{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss \n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import LabelEncoder,RobustScaler\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('Train.csv')\n",
    "to_drop=['user_id','MRG']\n",
    "train=train.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('Test.csv')\n",
    "to_drop=['user_id','MRG']\n",
    "test=test.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 0 to 399999\n",
      "Data columns (total 17 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   REGION          242480 non-null  object \n",
      " 1   TENURE          400000 non-null  object \n",
      " 2   MONTANT         259723 non-null  float64\n",
      " 3   FREQUENCE_RECH  259723 non-null  float64\n",
      " 4   REVENUE         265337 non-null  float64\n",
      " 5   ARPU_SEGMENT    265337 non-null  float64\n",
      " 6   FREQUENCE       265337 non-null  float64\n",
      " 7   DATA_VOLUME     203146 non-null  float64\n",
      " 8   ON_NET          254181 non-null  float64\n",
      " 9   ORANGE          233683 non-null  float64\n",
      " 10  TIGO            160614 non-null  float64\n",
      " 11  ZONE1           31690 non-null   float64\n",
      " 12  ZONE2           25513 non-null   float64\n",
      " 13  REGULARITY      400000 non-null  int64  \n",
      " 14  TOP_PACK        232671 non-null  object \n",
      " 15  FREQ_TOP_PACK   232671 non-null  float64\n",
      " 16  CHURN           400000 non-null  int64  \n",
      "dtypes: float64(12), int64(2), object(3)\n",
      "memory usage: 51.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REGION            157520\n",
       "TENURE                 0\n",
       "MONTANT           140277\n",
       "FREQUENCE_RECH    140277\n",
       "REVENUE           134663\n",
       "ARPU_SEGMENT      134663\n",
       "FREQUENCE         134663\n",
       "DATA_VOLUME       196854\n",
       "ON_NET            145819\n",
       "ORANGE            166317\n",
       "TIGO              239386\n",
       "ZONE1             368310\n",
       "ZONE2             374487\n",
       "REGULARITY             0\n",
       "TOP_PACK          167329\n",
       "FREQ_TOP_PACK     167329\n",
       "CHURN                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lag_features=['REVENUE','ARPU_SEGMENT']\n",
    "\n",
    "for feat in lag_features:\n",
    "    for i in range(3):\n",
    "        train[feat+'_lag'+str(i+1)] = train[feat].shift(i+1)\n",
    "        test[feat+'_lag'+str(i+1)] = test[feat].shift(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TOP_PACK']=train['TOP_PACK'].fillna('None')\n",
    "train['REGION']=train['REGION'].fillna('None')\n",
    "\n",
    "test['TOP_PACK']=test['TOP_PACK'].fillna('None')\n",
    "test['REGION']=test['REGION'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['MONTANT','FREQUENCE_RECH','REVENUE',\n",
    "          'ARPU_SEGMENT','FREQUENCE','DATA_VOLUME',\n",
    "         'ON_NET','ORANGE','TIGO','ZONE1','ZONE2','FREQ_TOP_PACK']:\n",
    "    train[i]=train[i].fillna(-9999)\n",
    "    test[i]=test[i].fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REGION', 'TENURE', 'TOP_PACK'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "cat_cols = train.select_dtypes(include='object').columns\n",
    "cat_cols\n",
    "\n",
    "catt_cols = test.select_dtypes(include='object').columns\n",
    "catt_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cat_cols:\n",
    "    train[i]=le.fit_transform(train[i])\n",
    "for i in catt_cols:\n",
    "    test[i]=le.fit_transform(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REGION            0\n",
       "TENURE            0\n",
       "MONTANT           0\n",
       "FREQUENCE_RECH    0\n",
       "REVENUE           0\n",
       "ARPU_SEGMENT      0\n",
       "FREQUENCE         0\n",
       "DATA_VOLUME       0\n",
       "ON_NET            0\n",
       "ORANGE            0\n",
       "TIGO              0\n",
       "ZONE1             0\n",
       "ZONE2             0\n",
       "REGULARITY        0\n",
       "TOP_PACK          0\n",
       "FREQ_TOP_PACK     0\n",
       "CHURN             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>TENURE</th>\n",
       "      <th>MONTANT</th>\n",
       "      <th>FREQUENCE_RECH</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>ARPU_SEGMENT</th>\n",
       "      <th>FREQUENCE</th>\n",
       "      <th>DATA_VOLUME</th>\n",
       "      <th>ON_NET</th>\n",
       "      <th>ORANGE</th>\n",
       "      <th>TIGO</th>\n",
       "      <th>ZONE1</th>\n",
       "      <th>ZONE2</th>\n",
       "      <th>REGULARITY</th>\n",
       "      <th>TOP_PACK</th>\n",
       "      <th>FREQ_TOP_PACK</th>\n",
       "      <th>CHURN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>15</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4427.0</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>832.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>85</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   REGION  TENURE  MONTANT  FREQUENCE_RECH  REVENUE  ARPU_SEGMENT  FREQUENCE  \\\n",
       "0      10       7  17000.0            32.0  18000.0        6000.0       34.0   \n",
       "1       9       7   4300.0            29.0   4427.0        1476.0       37.0   \n",
       "2      12       7   1500.0             3.0   1500.0         500.0        3.0   \n",
       "3       2       7   1500.0             3.0   2497.0         832.0        4.0   \n",
       "4       2       7      0.0             0.0    498.0         166.0        3.0   \n",
       "\n",
       "   DATA_VOLUME  ON_NET  ORANGE  TIGO  ZONE1  ZONE2  REGULARITY  TOP_PACK  \\\n",
       "0          0.0    97.0   355.0   6.0    0.0    0.0          62        15   \n",
       "1       1764.0     8.0     3.0   0.0    0.0    2.0          40        25   \n",
       "2          0.0    30.0    30.0   0.0    0.0    0.0          32        15   \n",
       "3          0.0   159.0    45.0  19.0    0.0    0.0          18        85   \n",
       "4          1.0     1.0     3.0   0.0    0.0    0.0          50        83   \n",
       "\n",
       "   FREQ_TOP_PACK  CHURN  \n",
       "0           35.0      0  \n",
       "1           22.0      0  \n",
       "2            3.0      0  \n",
       "3            3.0      0  \n",
       "4            0.0      0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(train.drop(['CHURN'],1))\n",
    "y=np.array(train['CHURN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "rs=RobustScaler()\n",
    "x_train=rs.fit_transform(x_train)\n",
    "x_test=rs.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300000, 16), (300000,), (100000, 16), (100000,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l1: 0.283379\tvalid_1's l1: 0.283106\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266186\tvalid_1's l1: 0.265722\n",
      "[3]\ttraining's l1: 0.251641\tvalid_1's l1: 0.251047\n",
      "[4]\ttraining's l1: 0.239246\tvalid_1's l1: 0.23852\n",
      "[5]\ttraining's l1: 0.228646\tvalid_1's l1: 0.227792\n",
      "[6]\ttraining's l1: 0.219581\tvalid_1's l1: 0.218633\n",
      "[7]\ttraining's l1: 0.211817\tvalid_1's l1: 0.210788\n",
      "[8]\ttraining's l1: 0.205152\tvalid_1's l1: 0.204073\n",
      "[9]\ttraining's l1: 0.199437\tvalid_1's l1: 0.198301\n",
      "[10]\ttraining's l1: 0.194517\tvalid_1's l1: 0.193326\n",
      "[11]\ttraining's l1: 0.190305\tvalid_1's l1: 0.189078\n",
      "[12]\ttraining's l1: 0.186692\tvalid_1's l1: 0.185433\n",
      "[13]\ttraining's l1: 0.183565\tvalid_1's l1: 0.182284\n",
      "[14]\ttraining's l1: 0.180917\tvalid_1's l1: 0.179631\n",
      "[15]\ttraining's l1: 0.178634\tvalid_1's l1: 0.177341\n",
      "[16]\ttraining's l1: 0.176716\tvalid_1's l1: 0.17543\n",
      "[17]\ttraining's l1: 0.175019\tvalid_1's l1: 0.17373\n",
      "[18]\ttraining's l1: 0.173572\tvalid_1's l1: 0.172278\n",
      "[19]\ttraining's l1: 0.172305\tvalid_1's l1: 0.171015\n",
      "[20]\ttraining's l1: 0.171247\tvalid_1's l1: 0.169961\n",
      "[21]\ttraining's l1: 0.170287\tvalid_1's l1: 0.169002\n",
      "[22]\ttraining's l1: 0.169488\tvalid_1's l1: 0.168212\n",
      "[23]\ttraining's l1: 0.168856\tvalid_1's l1: 0.167595\n",
      "[24]\ttraining's l1: 0.168248\tvalid_1's l1: 0.166998\n",
      "[25]\ttraining's l1: 0.167737\tvalid_1's l1: 0.166503\n",
      "[26]\ttraining's l1: 0.167323\tvalid_1's l1: 0.166101\n",
      "[27]\ttraining's l1: 0.166942\tvalid_1's l1: 0.165731\n",
      "[28]\ttraining's l1: 0.166637\tvalid_1's l1: 0.165465\n",
      "[29]\ttraining's l1: 0.166352\tvalid_1's l1: 0.165189\n",
      "[30]\ttraining's l1: 0.166085\tvalid_1's l1: 0.164948\n",
      "[31]\ttraining's l1: 0.165853\tvalid_1's l1: 0.164727\n",
      "[32]\ttraining's l1: 0.165664\tvalid_1's l1: 0.164545\n",
      "[33]\ttraining's l1: 0.165505\tvalid_1's l1: 0.164427\n",
      "[34]\ttraining's l1: 0.165353\tvalid_1's l1: 0.164307\n",
      "[35]\ttraining's l1: 0.165294\tvalid_1's l1: 0.164273\n",
      "[36]\ttraining's l1: 0.165199\tvalid_1's l1: 0.164192\n",
      "[37]\ttraining's l1: 0.165078\tvalid_1's l1: 0.164077\n",
      "[38]\ttraining's l1: 0.165003\tvalid_1's l1: 0.164011\n",
      "[39]\ttraining's l1: 0.164901\tvalid_1's l1: 0.163925\n",
      "[40]\ttraining's l1: 0.164787\tvalid_1's l1: 0.16382\n",
      "[41]\ttraining's l1: 0.164677\tvalid_1's l1: 0.16373\n",
      "[42]\ttraining's l1: 0.164628\tvalid_1's l1: 0.163705\n",
      "[43]\ttraining's l1: 0.164579\tvalid_1's l1: 0.163698\n",
      "[44]\ttraining's l1: 0.16452\tvalid_1's l1: 0.163677\n",
      "[45]\ttraining's l1: 0.164426\tvalid_1's l1: 0.163623\n",
      "[46]\ttraining's l1: 0.164425\tvalid_1's l1: 0.163632\n",
      "[47]\ttraining's l1: 0.164504\tvalid_1's l1: 0.163742\n",
      "[48]\ttraining's l1: 0.164409\tvalid_1's l1: 0.163646\n",
      "[49]\ttraining's l1: 0.164358\tvalid_1's l1: 0.163607\n",
      "[50]\ttraining's l1: 0.164314\tvalid_1's l1: 0.163609\n",
      "[51]\ttraining's l1: 0.164273\tvalid_1's l1: 0.163579\n",
      "[52]\ttraining's l1: 0.164308\tvalid_1's l1: 0.163618\n",
      "[53]\ttraining's l1: 0.164257\tvalid_1's l1: 0.163571\n",
      "[54]\ttraining's l1: 0.164372\tvalid_1's l1: 0.163714\n",
      "[55]\ttraining's l1: 0.164209\tvalid_1's l1: 0.163535\n",
      "[56]\ttraining's l1: 0.164392\tvalid_1's l1: 0.163779\n",
      "[57]\ttraining's l1: 0.164252\tvalid_1's l1: 0.163676\n",
      "[58]\ttraining's l1: 0.164359\tvalid_1's l1: 0.163806\n",
      "[59]\ttraining's l1: 0.164246\tvalid_1's l1: 0.163608\n",
      "[60]\ttraining's l1: 0.164322\tvalid_1's l1: 0.163788\n",
      "[61]\ttraining's l1: 0.164107\tvalid_1's l1: 0.163586\n",
      "[62]\ttraining's l1: 0.164123\tvalid_1's l1: 0.16367\n",
      "[63]\ttraining's l1: 0.164193\tvalid_1's l1: 0.163737\n",
      "[64]\ttraining's l1: 0.164076\tvalid_1's l1: 0.163653\n",
      "[65]\ttraining's l1: 0.164009\tvalid_1's l1: 0.163627\n",
      "[66]\ttraining's l1: 0.164073\tvalid_1's l1: 0.16374\n",
      "[67]\ttraining's l1: 0.163932\tvalid_1's l1: 0.163589\n",
      "[68]\ttraining's l1: 0.16389\tvalid_1's l1: 0.163579\n",
      "[69]\ttraining's l1: 0.163843\tvalid_1's l1: 0.16358\n",
      "[70]\ttraining's l1: 0.163782\tvalid_1's l1: 0.163545\n",
      "[71]\ttraining's l1: 0.163909\tvalid_1's l1: 0.163766\n",
      "[72]\ttraining's l1: 0.163858\tvalid_1's l1: 0.163685\n",
      "[73]\ttraining's l1: 0.163794\tvalid_1's l1: 0.163574\n",
      "[74]\ttraining's l1: 0.163971\tvalid_1's l1: 0.163749\n",
      "[75]\ttraining's l1: 0.163775\tvalid_1's l1: 0.163619\n",
      "[76]\ttraining's l1: 0.163993\tvalid_1's l1: 0.16379\n",
      "[77]\ttraining's l1: 0.163842\tvalid_1's l1: 0.163712\n",
      "[78]\ttraining's l1: 0.163813\tvalid_1's l1: 0.163694\n",
      "[79]\ttraining's l1: 0.163841\tvalid_1's l1: 0.163785\n",
      "[80]\ttraining's l1: 0.163703\tvalid_1's l1: 0.163622\n",
      "[81]\ttraining's l1: 0.163688\tvalid_1's l1: 0.16362\n",
      "[82]\ttraining's l1: 0.163808\tvalid_1's l1: 0.163804\n",
      "[83]\ttraining's l1: 0.163769\tvalid_1's l1: 0.16372\n",
      "[84]\ttraining's l1: 0.163751\tvalid_1's l1: 0.163667\n",
      "[85]\ttraining's l1: 0.163607\tvalid_1's l1: 0.163585\n",
      "[86]\ttraining's l1: 0.163586\tvalid_1's l1: 0.163577\n",
      "[87]\ttraining's l1: 0.163762\tvalid_1's l1: 0.16378\n",
      "[88]\ttraining's l1: 0.163887\tvalid_1's l1: 0.163931\n",
      "[89]\ttraining's l1: 0.163616\tvalid_1's l1: 0.1637\n",
      "[90]\ttraining's l1: 0.163767\tvalid_1's l1: 0.163857\n",
      "[91]\ttraining's l1: 0.163646\tvalid_1's l1: 0.163702\n",
      "[92]\ttraining's l1: 0.163489\tvalid_1's l1: 0.163535\n",
      "[93]\ttraining's l1: 0.16351\tvalid_1's l1: 0.163653\n",
      "[94]\ttraining's l1: 0.163436\tvalid_1's l1: 0.163602\n",
      "[95]\ttraining's l1: 0.163372\tvalid_1's l1: 0.16355\n",
      "[96]\ttraining's l1: 0.163354\tvalid_1's l1: 0.163539\n",
      "[97]\ttraining's l1: 0.1634\tvalid_1's l1: 0.16362\n",
      "[98]\ttraining's l1: 0.163398\tvalid_1's l1: 0.163544\n",
      "[99]\ttraining's l1: 0.163673\tvalid_1's l1: 0.16394\n",
      "[100]\ttraining's l1: 0.163424\tvalid_1's l1: 0.163581\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's l1: 0.163354\tvalid_1's l1: 0.163539\n",
      "logloss:  0.26276251060439637\n",
      "[1]\ttraining's l1: 0.283305\tvalid_1's l1: 0.283361\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266055\tvalid_1's l1: 0.2662\n",
      "[3]\ttraining's l1: 0.251447\tvalid_1's l1: 0.251666\n",
      "[4]\ttraining's l1: 0.239016\tvalid_1's l1: 0.239293\n",
      "[5]\ttraining's l1: 0.228386\tvalid_1's l1: 0.22871\n",
      "[6]\ttraining's l1: 0.219298\tvalid_1's l1: 0.219676\n",
      "[7]\ttraining's l1: 0.211527\tvalid_1's l1: 0.211929\n",
      "[8]\ttraining's l1: 0.204837\tvalid_1's l1: 0.205271\n",
      "[9]\ttraining's l1: 0.199096\tvalid_1's l1: 0.199553\n",
      "[10]\ttraining's l1: 0.194198\tvalid_1's l1: 0.194685\n",
      "[11]\ttraining's l1: 0.18996\tvalid_1's l1: 0.190478\n",
      "[12]\ttraining's l1: 0.186333\tvalid_1's l1: 0.186864\n",
      "[13]\ttraining's l1: 0.183202\tvalid_1's l1: 0.183757\n",
      "[14]\ttraining's l1: 0.180531\tvalid_1's l1: 0.181096\n",
      "[15]\ttraining's l1: 0.178227\tvalid_1's l1: 0.178814\n",
      "[16]\ttraining's l1: 0.176251\tvalid_1's l1: 0.17686\n",
      "[17]\ttraining's l1: 0.174525\tvalid_1's l1: 0.175155\n",
      "[18]\ttraining's l1: 0.173041\tvalid_1's l1: 0.173695\n",
      "[19]\ttraining's l1: 0.171804\tvalid_1's l1: 0.172456\n",
      "[20]\ttraining's l1: 0.170742\tvalid_1's l1: 0.171401\n",
      "[21]\ttraining's l1: 0.169794\tvalid_1's l1: 0.170473\n",
      "[22]\ttraining's l1: 0.168991\tvalid_1's l1: 0.169678\n",
      "[23]\ttraining's l1: 0.168304\tvalid_1's l1: 0.169015\n",
      "[24]\ttraining's l1: 0.167745\tvalid_1's l1: 0.168472\n",
      "[25]\ttraining's l1: 0.167278\tvalid_1's l1: 0.16804\n",
      "[26]\ttraining's l1: 0.166831\tvalid_1's l1: 0.167607\n",
      "[27]\ttraining's l1: 0.166467\tvalid_1's l1: 0.167259\n",
      "[28]\ttraining's l1: 0.166107\tvalid_1's l1: 0.166909\n",
      "[29]\ttraining's l1: 0.165865\tvalid_1's l1: 0.166692\n",
      "[30]\ttraining's l1: 0.165567\tvalid_1's l1: 0.166413\n",
      "[31]\ttraining's l1: 0.165351\tvalid_1's l1: 0.16621\n",
      "[32]\ttraining's l1: 0.165184\tvalid_1's l1: 0.166067\n",
      "[33]\ttraining's l1: 0.165037\tvalid_1's l1: 0.165954\n",
      "[34]\ttraining's l1: 0.164919\tvalid_1's l1: 0.165852\n",
      "[35]\ttraining's l1: 0.164826\tvalid_1's l1: 0.165757\n",
      "[36]\ttraining's l1: 0.16472\tvalid_1's l1: 0.165664\n",
      "[37]\ttraining's l1: 0.164625\tvalid_1's l1: 0.165579\n",
      "[38]\ttraining's l1: 0.16457\tvalid_1's l1: 0.16555\n",
      "[39]\ttraining's l1: 0.164506\tvalid_1's l1: 0.165499\n",
      "[40]\ttraining's l1: 0.164409\tvalid_1's l1: 0.165416\n",
      "[41]\ttraining's l1: 0.164287\tvalid_1's l1: 0.165305\n",
      "[42]\ttraining's l1: 0.164249\tvalid_1's l1: 0.165257\n",
      "[43]\ttraining's l1: 0.164161\tvalid_1's l1: 0.16519\n",
      "[44]\ttraining's l1: 0.164107\tvalid_1's l1: 0.165141\n",
      "[45]\ttraining's l1: 0.163993\tvalid_1's l1: 0.165061\n",
      "[46]\ttraining's l1: 0.163948\tvalid_1's l1: 0.165\n",
      "[47]\ttraining's l1: 0.163847\tvalid_1's l1: 0.164973\n",
      "[48]\ttraining's l1: 0.163811\tvalid_1's l1: 0.164944\n",
      "[49]\ttraining's l1: 0.163745\tvalid_1's l1: 0.164885\n",
      "[50]\ttraining's l1: 0.163759\tvalid_1's l1: 0.164893\n",
      "[51]\ttraining's l1: 0.163841\tvalid_1's l1: 0.165048\n",
      "[52]\ttraining's l1: 0.163703\tvalid_1's l1: 0.16489\n",
      "[53]\ttraining's l1: 0.163645\tvalid_1's l1: 0.164861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54]\ttraining's l1: 0.163835\tvalid_1's l1: 0.165046\n",
      "[55]\ttraining's l1: 0.163685\tvalid_1's l1: 0.164917\n",
      "[56]\ttraining's l1: 0.16372\tvalid_1's l1: 0.164914\n",
      "[57]\ttraining's l1: 0.163598\tvalid_1's l1: 0.164875\n",
      "[58]\ttraining's l1: 0.163625\tvalid_1's l1: 0.164945\n",
      "[59]\ttraining's l1: 0.163744\tvalid_1's l1: 0.165078\n",
      "[60]\ttraining's l1: 0.163724\tvalid_1's l1: 0.165012\n",
      "[61]\ttraining's l1: 0.163561\tvalid_1's l1: 0.164931\n",
      "[62]\ttraining's l1: 0.163926\tvalid_1's l1: 0.165249\n",
      "[63]\ttraining's l1: 0.163707\tvalid_1's l1: 0.165085\n",
      "[64]\ttraining's l1: 0.163595\tvalid_1's l1: 0.165028\n",
      "[65]\ttraining's l1: 0.163541\tvalid_1's l1: 0.165007\n",
      "[66]\ttraining's l1: 0.163504\tvalid_1's l1: 0.164991\n",
      "[67]\ttraining's l1: 0.16377\tvalid_1's l1: 0.16554\n",
      "[68]\ttraining's l1: 0.163469\tvalid_1's l1: 0.165039\n",
      "[69]\ttraining's l1: 0.163512\tvalid_1's l1: 0.165114\n",
      "[70]\ttraining's l1: 0.163475\tvalid_1's l1: 0.165105\n",
      "[71]\ttraining's l1: 0.163452\tvalid_1's l1: 0.165045\n",
      "[72]\ttraining's l1: 0.163362\tvalid_1's l1: 0.164947\n",
      "[73]\ttraining's l1: 0.163308\tvalid_1's l1: 0.164935\n",
      "[74]\ttraining's l1: 0.16328\tvalid_1's l1: 0.164924\n",
      "[75]\ttraining's l1: 0.163259\tvalid_1's l1: 0.164938\n",
      "[76]\ttraining's l1: 0.163305\tvalid_1's l1: 0.164994\n",
      "[77]\ttraining's l1: 0.163517\tvalid_1's l1: 0.165199\n",
      "[78]\ttraining's l1: 0.16323\tvalid_1's l1: 0.164943\n",
      "[79]\ttraining's l1: 0.163203\tvalid_1's l1: 0.164937\n",
      "[80]\ttraining's l1: 0.163238\tvalid_1's l1: 0.164951\n",
      "[81]\ttraining's l1: 0.163207\tvalid_1's l1: 0.165052\n",
      "[82]\ttraining's l1: 0.163243\tvalid_1's l1: 0.165046\n",
      "[83]\ttraining's l1: 0.163178\tvalid_1's l1: 0.165087\n",
      "[84]\ttraining's l1: 0.163181\tvalid_1's l1: 0.165188\n",
      "[85]\ttraining's l1: 0.163206\tvalid_1's l1: 0.165208\n",
      "[86]\ttraining's l1: 0.163044\tvalid_1's l1: 0.165017\n",
      "[87]\ttraining's l1: 0.163158\tvalid_1's l1: 0.165075\n",
      "[88]\ttraining's l1: 0.163198\tvalid_1's l1: 0.16538\n",
      "[89]\ttraining's l1: 0.16343\tvalid_1's l1: 0.165632\n",
      "[90]\ttraining's l1: 0.163084\tvalid_1's l1: 0.165272\n",
      "[91]\ttraining's l1: 0.163058\tvalid_1's l1: 0.165243\n",
      "[92]\ttraining's l1: 0.163108\tvalid_1's l1: 0.165283\n",
      "[93]\ttraining's l1: 0.163006\tvalid_1's l1: 0.165183\n",
      "[94]\ttraining's l1: 0.163091\tvalid_1's l1: 0.165242\n",
      "[95]\ttraining's l1: 0.162969\tvalid_1's l1: 0.165144\n",
      "[96]\ttraining's l1: 0.162936\tvalid_1's l1: 0.165128\n",
      "[97]\ttraining's l1: 0.163007\tvalid_1's l1: 0.165348\n",
      "[98]\ttraining's l1: 0.163577\tvalid_1's l1: 0.165867\n",
      "[99]\ttraining's l1: 0.163031\tvalid_1's l1: 0.165293\n",
      "[100]\ttraining's l1: 0.163354\tvalid_1's l1: 0.165588\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's l1: 0.162936\tvalid_1's l1: 0.165128\n",
      "logloss:  0.2668591051895416\n",
      "[1]\ttraining's l1: 0.283292\tvalid_1's l1: 0.28352\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266047\tvalid_1's l1: 0.266442\n",
      "[3]\ttraining's l1: 0.251433\tvalid_1's l1: 0.251971\n",
      "[4]\ttraining's l1: 0.23899\tvalid_1's l1: 0.239666\n",
      "[5]\ttraining's l1: 0.228367\tvalid_1's l1: 0.229149\n",
      "[6]\ttraining's l1: 0.219265\tvalid_1's l1: 0.220122\n",
      "[7]\ttraining's l1: 0.211482\tvalid_1's l1: 0.212404\n",
      "[8]\ttraining's l1: 0.204799\tvalid_1's l1: 0.205789\n",
      "[9]\ttraining's l1: 0.199061\tvalid_1's l1: 0.200103\n",
      "[10]\ttraining's l1: 0.194152\tvalid_1's l1: 0.19525\n",
      "[11]\ttraining's l1: 0.189938\tvalid_1's l1: 0.191098\n",
      "[12]\ttraining's l1: 0.186316\tvalid_1's l1: 0.187521\n",
      "[13]\ttraining's l1: 0.183214\tvalid_1's l1: 0.184456\n",
      "[14]\ttraining's l1: 0.180562\tvalid_1's l1: 0.181847\n",
      "[15]\ttraining's l1: 0.178282\tvalid_1's l1: 0.179601\n",
      "[16]\ttraining's l1: 0.176334\tvalid_1's l1: 0.177679\n",
      "[17]\ttraining's l1: 0.174611\tvalid_1's l1: 0.175987\n",
      "[18]\ttraining's l1: 0.173121\tvalid_1's l1: 0.174526\n",
      "[19]\ttraining's l1: 0.171847\tvalid_1's l1: 0.173281\n",
      "[20]\ttraining's l1: 0.170754\tvalid_1's l1: 0.172211\n",
      "[21]\ttraining's l1: 0.169836\tvalid_1's l1: 0.171317\n",
      "[22]\ttraining's l1: 0.169025\tvalid_1's l1: 0.170525\n",
      "[23]\ttraining's l1: 0.168317\tvalid_1's l1: 0.169842\n",
      "[24]\ttraining's l1: 0.167749\tvalid_1's l1: 0.169291\n",
      "[25]\ttraining's l1: 0.167252\tvalid_1's l1: 0.168825\n",
      "[26]\ttraining's l1: 0.166795\tvalid_1's l1: 0.168372\n",
      "[27]\ttraining's l1: 0.16643\tvalid_1's l1: 0.168039\n",
      "[28]\ttraining's l1: 0.166101\tvalid_1's l1: 0.167714\n",
      "[29]\ttraining's l1: 0.165829\tvalid_1's l1: 0.16746\n",
      "[30]\ttraining's l1: 0.165549\tvalid_1's l1: 0.16719\n",
      "[31]\ttraining's l1: 0.165316\tvalid_1's l1: 0.166956\n",
      "[32]\ttraining's l1: 0.16516\tvalid_1's l1: 0.166818\n",
      "[33]\ttraining's l1: 0.165004\tvalid_1's l1: 0.166667\n",
      "[34]\ttraining's l1: 0.164897\tvalid_1's l1: 0.166584\n",
      "[35]\ttraining's l1: 0.164777\tvalid_1's l1: 0.16648\n",
      "[36]\ttraining's l1: 0.164657\tvalid_1's l1: 0.16637\n",
      "[37]\ttraining's l1: 0.164528\tvalid_1's l1: 0.166257\n",
      "[38]\ttraining's l1: 0.164446\tvalid_1's l1: 0.16617\n",
      "[39]\ttraining's l1: 0.164293\tvalid_1's l1: 0.166057\n",
      "[40]\ttraining's l1: 0.164329\tvalid_1's l1: 0.166089\n",
      "[41]\ttraining's l1: 0.164267\tvalid_1's l1: 0.166012\n",
      "[42]\ttraining's l1: 0.164212\tvalid_1's l1: 0.165984\n",
      "[43]\ttraining's l1: 0.164165\tvalid_1's l1: 0.165959\n",
      "[44]\ttraining's l1: 0.164131\tvalid_1's l1: 0.165946\n",
      "[45]\ttraining's l1: 0.164085\tvalid_1's l1: 0.165946\n",
      "[46]\ttraining's l1: 0.164161\tvalid_1's l1: 0.165996\n",
      "[47]\ttraining's l1: 0.164135\tvalid_1's l1: 0.165969\n",
      "[48]\ttraining's l1: 0.16403\tvalid_1's l1: 0.165897\n",
      "[49]\ttraining's l1: 0.163994\tvalid_1's l1: 0.165865\n",
      "[50]\ttraining's l1: 0.163901\tvalid_1's l1: 0.165816\n",
      "[51]\ttraining's l1: 0.164165\tvalid_1's l1: 0.166223\n",
      "[52]\ttraining's l1: 0.164067\tvalid_1's l1: 0.166108\n",
      "[53]\ttraining's l1: 0.164042\tvalid_1's l1: 0.165976\n",
      "[54]\ttraining's l1: 0.163919\tvalid_1's l1: 0.165918\n",
      "[55]\ttraining's l1: 0.164014\tvalid_1's l1: 0.165971\n",
      "[56]\ttraining's l1: 0.163915\tvalid_1's l1: 0.166003\n",
      "[57]\ttraining's l1: 0.164\tvalid_1's l1: 0.166065\n",
      "[58]\ttraining's l1: 0.163958\tvalid_1's l1: 0.166047\n",
      "[59]\ttraining's l1: 0.163935\tvalid_1's l1: 0.165911\n",
      "[60]\ttraining's l1: 0.163841\tvalid_1's l1: 0.16599\n",
      "[61]\ttraining's l1: 0.163807\tvalid_1's l1: 0.165974\n",
      "[62]\ttraining's l1: 0.163795\tvalid_1's l1: 0.166009\n",
      "[63]\ttraining's l1: 0.163883\tvalid_1's l1: 0.165993\n",
      "[64]\ttraining's l1: 0.163719\tvalid_1's l1: 0.165938\n",
      "[65]\ttraining's l1: 0.163653\tvalid_1's l1: 0.165906\n",
      "[66]\ttraining's l1: 0.163571\tvalid_1's l1: 0.165868\n",
      "[67]\ttraining's l1: 0.163538\tvalid_1's l1: 0.165851\n",
      "[68]\ttraining's l1: 0.164163\tvalid_1's l1: 0.166304\n",
      "[69]\ttraining's l1: 0.163525\tvalid_1's l1: 0.165797\n",
      "[70]\ttraining's l1: 0.163524\tvalid_1's l1: 0.165797\n",
      "[71]\ttraining's l1: 0.163524\tvalid_1's l1: 0.16584\n",
      "[72]\ttraining's l1: 0.163503\tvalid_1's l1: 0.165837\n",
      "[73]\ttraining's l1: 0.163535\tvalid_1's l1: 0.165951\n",
      "[74]\ttraining's l1: 0.163805\tvalid_1's l1: 0.166013\n",
      "[75]\ttraining's l1: 0.163611\tvalid_1's l1: 0.165904\n",
      "[76]\ttraining's l1: 0.163541\tvalid_1's l1: 0.16591\n",
      "[77]\ttraining's l1: 0.164136\tvalid_1's l1: 0.16655\n",
      "[78]\ttraining's l1: 0.163545\tvalid_1's l1: 0.165956\n",
      "[79]\ttraining's l1: 0.163491\tvalid_1's l1: 0.165915\n",
      "[80]\ttraining's l1: 0.163451\tvalid_1's l1: 0.165926\n",
      "[81]\ttraining's l1: 0.163421\tvalid_1's l1: 0.165918\n",
      "[82]\ttraining's l1: 0.16341\tvalid_1's l1: 0.165936\n",
      "[83]\ttraining's l1: 0.163584\tvalid_1's l1: 0.166173\n",
      "[84]\ttraining's l1: 0.163431\tvalid_1's l1: 0.166003\n",
      "[85]\ttraining's l1: 0.163518\tvalid_1's l1: 0.166223\n",
      "[86]\ttraining's l1: 0.163379\tvalid_1's l1: 0.166051\n",
      "[87]\ttraining's l1: 0.163472\tvalid_1's l1: 0.166192\n",
      "[88]\ttraining's l1: 0.163496\tvalid_1's l1: 0.166171\n",
      "[89]\ttraining's l1: 0.163302\tvalid_1's l1: 0.166019\n",
      "[90]\ttraining's l1: 0.163341\tvalid_1's l1: 0.166107\n",
      "[91]\ttraining's l1: 0.163609\tvalid_1's l1: 0.166301\n",
      "[92]\ttraining's l1: 0.163332\tvalid_1's l1: 0.166031\n",
      "[93]\ttraining's l1: 0.163238\tvalid_1's l1: 0.166011\n",
      "[94]\ttraining's l1: 0.16332\tvalid_1's l1: 0.166178\n",
      "[95]\ttraining's l1: 0.16322\tvalid_1's l1: 0.166049\n",
      "[96]\ttraining's l1: 0.163056\tvalid_1's l1: 0.165882\n",
      "[97]\ttraining's l1: 0.16307\tvalid_1's l1: 0.165979\n",
      "[98]\ttraining's l1: 0.163137\tvalid_1's l1: 0.16601\n",
      "[99]\ttraining's l1: 0.163154\tvalid_1's l1: 0.166128\n",
      "[100]\ttraining's l1: 0.163656\tvalid_1's l1: 0.16675\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's l1: 0.163056\tvalid_1's l1: 0.165882\n",
      "logloss:  0.2693073979944298\n",
      "[1]\ttraining's l1: 0.283354\tvalid_1's l1: 0.283171\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266137\tvalid_1's l1: 0.265863\n",
      "[3]\ttraining's l1: 0.251552\tvalid_1's l1: 0.251217\n",
      "[4]\ttraining's l1: 0.239148\tvalid_1's l1: 0.238758\n",
      "[5]\ttraining's l1: 0.228547\tvalid_1's l1: 0.228098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\ttraining's l1: 0.219483\tvalid_1's l1: 0.218983\n",
      "[7]\ttraining's l1: 0.211711\tvalid_1's l1: 0.211174\n",
      "[8]\ttraining's l1: 0.205043\tvalid_1's l1: 0.204465\n",
      "[9]\ttraining's l1: 0.199317\tvalid_1's l1: 0.198717\n",
      "[10]\ttraining's l1: 0.1944\tvalid_1's l1: 0.193801\n",
      "[11]\ttraining's l1: 0.190198\tvalid_1's l1: 0.189601\n",
      "[12]\ttraining's l1: 0.18657\tvalid_1's l1: 0.18597\n",
      "[13]\ttraining's l1: 0.18347\tvalid_1's l1: 0.182879\n",
      "[14]\ttraining's l1: 0.18081\tvalid_1's l1: 0.180209\n",
      "[15]\ttraining's l1: 0.178519\tvalid_1's l1: 0.177924\n",
      "[16]\ttraining's l1: 0.176543\tvalid_1's l1: 0.175955\n",
      "[17]\ttraining's l1: 0.174837\tvalid_1's l1: 0.174262\n",
      "[18]\ttraining's l1: 0.173381\tvalid_1's l1: 0.17282\n",
      "[19]\ttraining's l1: 0.172102\tvalid_1's l1: 0.171552\n",
      "[20]\ttraining's l1: 0.171034\tvalid_1's l1: 0.1705\n",
      "[21]\ttraining's l1: 0.170091\tvalid_1's l1: 0.169568\n",
      "[22]\ttraining's l1: 0.169302\tvalid_1's l1: 0.168797\n",
      "[23]\ttraining's l1: 0.168634\tvalid_1's l1: 0.168143\n",
      "[24]\ttraining's l1: 0.168056\tvalid_1's l1: 0.167572\n",
      "[25]\ttraining's l1: 0.16754\tvalid_1's l1: 0.167065\n",
      "[26]\ttraining's l1: 0.167096\tvalid_1's l1: 0.166635\n",
      "[27]\ttraining's l1: 0.166731\tvalid_1's l1: 0.166283\n",
      "[28]\ttraining's l1: 0.166392\tvalid_1's l1: 0.165964\n",
      "[29]\ttraining's l1: 0.166103\tvalid_1's l1: 0.16569\n",
      "[30]\ttraining's l1: 0.165848\tvalid_1's l1: 0.165455\n",
      "[31]\ttraining's l1: 0.165645\tvalid_1's l1: 0.165268\n",
      "[32]\ttraining's l1: 0.165473\tvalid_1's l1: 0.165106\n",
      "[33]\ttraining's l1: 0.165292\tvalid_1's l1: 0.164956\n",
      "[34]\ttraining's l1: 0.165175\tvalid_1's l1: 0.164841\n",
      "[35]\ttraining's l1: 0.165044\tvalid_1's l1: 0.16473\n",
      "[36]\ttraining's l1: 0.164909\tvalid_1's l1: 0.164625\n",
      "[37]\ttraining's l1: 0.164818\tvalid_1's l1: 0.16455\n",
      "[38]\ttraining's l1: 0.164783\tvalid_1's l1: 0.16452\n",
      "[39]\ttraining's l1: 0.164742\tvalid_1's l1: 0.164485\n",
      "[40]\ttraining's l1: 0.164693\tvalid_1's l1: 0.16444\n",
      "[41]\ttraining's l1: 0.164628\tvalid_1's l1: 0.164406\n",
      "[42]\ttraining's l1: 0.164627\tvalid_1's l1: 0.16441\n",
      "[43]\ttraining's l1: 0.164501\tvalid_1's l1: 0.164316\n",
      "[44]\ttraining's l1: 0.164458\tvalid_1's l1: 0.164322\n",
      "[45]\ttraining's l1: 0.164396\tvalid_1's l1: 0.164294\n",
      "[46]\ttraining's l1: 0.164413\tvalid_1's l1: 0.164329\n",
      "[47]\ttraining's l1: 0.164306\tvalid_1's l1: 0.164241\n",
      "[48]\ttraining's l1: 0.164262\tvalid_1's l1: 0.164203\n",
      "[49]\ttraining's l1: 0.164265\tvalid_1's l1: 0.164184\n",
      "[50]\ttraining's l1: 0.164333\tvalid_1's l1: 0.164375\n",
      "[51]\ttraining's l1: 0.164384\tvalid_1's l1: 0.164441\n",
      "[52]\ttraining's l1: 0.164184\tvalid_1's l1: 0.1642\n",
      "[53]\ttraining's l1: 0.164278\tvalid_1's l1: 0.164241\n",
      "[54]\ttraining's l1: 0.164092\tvalid_1's l1: 0.164123\n",
      "[55]\ttraining's l1: 0.164415\tvalid_1's l1: 0.164538\n",
      "[56]\ttraining's l1: 0.16426\tvalid_1's l1: 0.164331\n",
      "[57]\ttraining's l1: 0.164117\tvalid_1's l1: 0.164151\n",
      "[58]\ttraining's l1: 0.164251\tvalid_1's l1: 0.164409\n",
      "[59]\ttraining's l1: 0.164171\tvalid_1's l1: 0.164365\n",
      "[60]\ttraining's l1: 0.16409\tvalid_1's l1: 0.16417\n",
      "[61]\ttraining's l1: 0.164292\tvalid_1's l1: 0.164339\n",
      "[62]\ttraining's l1: 0.164048\tvalid_1's l1: 0.164232\n",
      "[63]\ttraining's l1: 0.164156\tvalid_1's l1: 0.164323\n",
      "[64]\ttraining's l1: 0.164029\tvalid_1's l1: 0.164166\n",
      "[65]\ttraining's l1: 0.164019\tvalid_1's l1: 0.164185\n",
      "[66]\ttraining's l1: 0.164045\tvalid_1's l1: 0.164197\n",
      "[67]\ttraining's l1: 0.16411\tvalid_1's l1: 0.164374\n",
      "[68]\ttraining's l1: 0.164044\tvalid_1's l1: 0.164417\n",
      "[69]\ttraining's l1: 0.164045\tvalid_1's l1: 0.164421\n",
      "[70]\ttraining's l1: 0.164044\tvalid_1's l1: 0.164358\n",
      "[71]\ttraining's l1: 0.163981\tvalid_1's l1: 0.164298\n",
      "[72]\ttraining's l1: 0.164025\tvalid_1's l1: 0.16443\n",
      "[73]\ttraining's l1: 0.163982\tvalid_1's l1: 0.164458\n",
      "[74]\ttraining's l1: 0.163988\tvalid_1's l1: 0.164448\n",
      "[75]\ttraining's l1: 0.163918\tvalid_1's l1: 0.164387\n",
      "[76]\ttraining's l1: 0.163932\tvalid_1's l1: 0.164463\n",
      "[77]\ttraining's l1: 0.163919\tvalid_1's l1: 0.164385\n",
      "[78]\ttraining's l1: 0.163903\tvalid_1's l1: 0.164352\n",
      "[79]\ttraining's l1: 0.163874\tvalid_1's l1: 0.164372\n",
      "[80]\ttraining's l1: 0.163836\tvalid_1's l1: 0.164356\n",
      "[81]\ttraining's l1: 0.163793\tvalid_1's l1: 0.164323\n",
      "[82]\ttraining's l1: 0.163718\tvalid_1's l1: 0.164273\n",
      "[83]\ttraining's l1: 0.163698\tvalid_1's l1: 0.164268\n",
      "[84]\ttraining's l1: 0.164127\tvalid_1's l1: 0.164495\n",
      "[85]\ttraining's l1: 0.163764\tvalid_1's l1: 0.164198\n",
      "[86]\ttraining's l1: 0.16371\tvalid_1's l1: 0.164142\n",
      "[87]\ttraining's l1: 0.163815\tvalid_1's l1: 0.164281\n",
      "[88]\ttraining's l1: 0.163647\tvalid_1's l1: 0.164127\n",
      "[89]\ttraining's l1: 0.16362\tvalid_1's l1: 0.164147\n",
      "[90]\ttraining's l1: 0.163608\tvalid_1's l1: 0.164157\n",
      "[91]\ttraining's l1: 0.163518\tvalid_1's l1: 0.164096\n",
      "[92]\ttraining's l1: 0.163502\tvalid_1's l1: 0.164094\n",
      "[93]\ttraining's l1: 0.163488\tvalid_1's l1: 0.164086\n",
      "[94]\ttraining's l1: 0.163576\tvalid_1's l1: 0.164299\n",
      "[95]\ttraining's l1: 0.163563\tvalid_1's l1: 0.164171\n",
      "[96]\ttraining's l1: 0.163467\tvalid_1's l1: 0.164169\n",
      "[97]\ttraining's l1: 0.16351\tvalid_1's l1: 0.16418\n",
      "[98]\ttraining's l1: 0.163429\tvalid_1's l1: 0.164096\n",
      "[99]\ttraining's l1: 0.1634\tvalid_1's l1: 0.164079\n",
      "[100]\ttraining's l1: 0.163377\tvalid_1's l1: 0.164074\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l1: 0.163377\tvalid_1's l1: 0.164074\n",
      "logloss:  0.26365833872271444\n",
      "[1]\ttraining's l1: 0.283297\tvalid_1's l1: 0.283316\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266055\tvalid_1's l1: 0.266076\n",
      "[3]\ttraining's l1: 0.251449\tvalid_1's l1: 0.251482\n",
      "[4]\ttraining's l1: 0.239027\tvalid_1's l1: 0.239078\n",
      "[5]\ttraining's l1: 0.228411\tvalid_1's l1: 0.228454\n",
      "[6]\ttraining's l1: 0.219323\tvalid_1's l1: 0.219386\n",
      "[7]\ttraining's l1: 0.211539\tvalid_1's l1: 0.211617\n",
      "[8]\ttraining's l1: 0.204863\tvalid_1's l1: 0.204948\n",
      "[9]\ttraining's l1: 0.199128\tvalid_1's l1: 0.199223\n",
      "[10]\ttraining's l1: 0.194229\tvalid_1's l1: 0.194332\n",
      "[11]\ttraining's l1: 0.190019\tvalid_1's l1: 0.190128\n",
      "[12]\ttraining's l1: 0.186409\tvalid_1's l1: 0.18652\n",
      "[13]\ttraining's l1: 0.183265\tvalid_1's l1: 0.183391\n",
      "[14]\ttraining's l1: 0.180623\tvalid_1's l1: 0.180764\n",
      "[15]\ttraining's l1: 0.178331\tvalid_1's l1: 0.178467\n",
      "[16]\ttraining's l1: 0.176366\tvalid_1's l1: 0.176503\n",
      "[17]\ttraining's l1: 0.174662\tvalid_1's l1: 0.174803\n",
      "[18]\ttraining's l1: 0.173187\tvalid_1's l1: 0.173332\n",
      "[19]\ttraining's l1: 0.171943\tvalid_1's l1: 0.172087\n",
      "[20]\ttraining's l1: 0.170885\tvalid_1's l1: 0.171043\n",
      "[21]\ttraining's l1: 0.169929\tvalid_1's l1: 0.17008\n",
      "[22]\ttraining's l1: 0.169105\tvalid_1's l1: 0.169273\n",
      "[23]\ttraining's l1: 0.168383\tvalid_1's l1: 0.168566\n",
      "[24]\ttraining's l1: 0.167785\tvalid_1's l1: 0.167975\n",
      "[25]\ttraining's l1: 0.167288\tvalid_1's l1: 0.16748\n",
      "[26]\ttraining's l1: 0.166855\tvalid_1's l1: 0.167066\n",
      "[27]\ttraining's l1: 0.166529\tvalid_1's l1: 0.166753\n",
      "[28]\ttraining's l1: 0.166213\tvalid_1's l1: 0.16645\n",
      "[29]\ttraining's l1: 0.16593\tvalid_1's l1: 0.16618\n",
      "[30]\ttraining's l1: 0.165637\tvalid_1's l1: 0.165903\n",
      "[31]\ttraining's l1: 0.165407\tvalid_1's l1: 0.165674\n",
      "[32]\ttraining's l1: 0.165222\tvalid_1's l1: 0.165496\n",
      "[33]\ttraining's l1: 0.165075\tvalid_1's l1: 0.165358\n",
      "[34]\ttraining's l1: 0.164946\tvalid_1's l1: 0.165235\n",
      "[35]\ttraining's l1: 0.164806\tvalid_1's l1: 0.165118\n",
      "[36]\ttraining's l1: 0.164679\tvalid_1's l1: 0.164997\n",
      "[37]\ttraining's l1: 0.164594\tvalid_1's l1: 0.164922\n",
      "[38]\ttraining's l1: 0.164466\tvalid_1's l1: 0.164811\n",
      "[39]\ttraining's l1: 0.16442\tvalid_1's l1: 0.164808\n",
      "[40]\ttraining's l1: 0.164317\tvalid_1's l1: 0.164704\n",
      "[41]\ttraining's l1: 0.164253\tvalid_1's l1: 0.164692\n",
      "[42]\ttraining's l1: 0.164376\tvalid_1's l1: 0.164847\n",
      "[43]\ttraining's l1: 0.164422\tvalid_1's l1: 0.164855\n",
      "[44]\ttraining's l1: 0.164366\tvalid_1's l1: 0.164842\n",
      "[45]\ttraining's l1: 0.164227\tvalid_1's l1: 0.164699\n",
      "[46]\ttraining's l1: 0.164191\tvalid_1's l1: 0.164676\n",
      "[47]\ttraining's l1: 0.164136\tvalid_1's l1: 0.164636\n",
      "[48]\ttraining's l1: 0.164297\tvalid_1's l1: 0.164917\n",
      "[49]\ttraining's l1: 0.164131\tvalid_1's l1: 0.164714\n",
      "[50]\ttraining's l1: 0.164111\tvalid_1's l1: 0.164714\n",
      "[51]\ttraining's l1: 0.164116\tvalid_1's l1: 0.164548\n",
      "[52]\ttraining's l1: 0.164201\tvalid_1's l1: 0.164956\n",
      "[53]\ttraining's l1: 0.164009\tvalid_1's l1: 0.164569\n",
      "[54]\ttraining's l1: 0.163987\tvalid_1's l1: 0.164631\n",
      "[55]\ttraining's l1: 0.163952\tvalid_1's l1: 0.164618\n",
      "[56]\ttraining's l1: 0.16394\tvalid_1's l1: 0.164501\n",
      "[57]\ttraining's l1: 0.163861\tvalid_1's l1: 0.164471\n",
      "[58]\ttraining's l1: 0.163843\tvalid_1's l1: 0.164516\n",
      "[59]\ttraining's l1: 0.163803\tvalid_1's l1: 0.164479\n",
      "[60]\ttraining's l1: 0.163838\tvalid_1's l1: 0.164492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61]\ttraining's l1: 0.16376\tvalid_1's l1: 0.164472\n",
      "[62]\ttraining's l1: 0.163831\tvalid_1's l1: 0.164542\n",
      "[63]\ttraining's l1: 0.163746\tvalid_1's l1: 0.164479\n",
      "[64]\ttraining's l1: 0.163734\tvalid_1's l1: 0.164453\n",
      "[65]\ttraining's l1: 0.163686\tvalid_1's l1: 0.164422\n",
      "[66]\ttraining's l1: 0.163605\tvalid_1's l1: 0.164355\n",
      "[67]\ttraining's l1: 0.163568\tvalid_1's l1: 0.164338\n",
      "[68]\ttraining's l1: 0.163518\tvalid_1's l1: 0.164301\n",
      "[69]\ttraining's l1: 0.163485\tvalid_1's l1: 0.164292\n",
      "[70]\ttraining's l1: 0.16351\tvalid_1's l1: 0.164395\n",
      "[71]\ttraining's l1: 0.163658\tvalid_1's l1: 0.164506\n",
      "[72]\ttraining's l1: 0.163438\tvalid_1's l1: 0.16429\n",
      "[73]\ttraining's l1: 0.163486\tvalid_1's l1: 0.164463\n",
      "[74]\ttraining's l1: 0.163384\tvalid_1's l1: 0.164264\n",
      "[75]\ttraining's l1: 0.163366\tvalid_1's l1: 0.164239\n",
      "[76]\ttraining's l1: 0.163332\tvalid_1's l1: 0.164216\n",
      "[77]\ttraining's l1: 0.163285\tvalid_1's l1: 0.164199\n",
      "[78]\ttraining's l1: 0.163388\tvalid_1's l1: 0.164266\n",
      "[79]\ttraining's l1: 0.163353\tvalid_1's l1: 0.164463\n",
      "[80]\ttraining's l1: 0.163419\tvalid_1's l1: 0.164554\n",
      "[81]\ttraining's l1: 0.163329\tvalid_1's l1: 0.164372\n",
      "[82]\ttraining's l1: 0.163244\tvalid_1's l1: 0.164316\n",
      "[83]\ttraining's l1: 0.1635\tvalid_1's l1: 0.164552\n",
      "[84]\ttraining's l1: 0.163404\tvalid_1's l1: 0.164495\n",
      "[85]\ttraining's l1: 0.163416\tvalid_1's l1: 0.164631\n",
      "[86]\ttraining's l1: 0.163559\tvalid_1's l1: 0.164819\n",
      "[87]\ttraining's l1: 0.16342\tvalid_1's l1: 0.164565\n",
      "[88]\ttraining's l1: 0.163356\tvalid_1's l1: 0.164455\n",
      "[89]\ttraining's l1: 0.163485\tvalid_1's l1: 0.16468\n",
      "[90]\ttraining's l1: 0.16356\tvalid_1's l1: 0.164875\n",
      "[91]\ttraining's l1: 0.163253\tvalid_1's l1: 0.164493\n",
      "[92]\ttraining's l1: 0.163856\tvalid_1's l1: 0.164997\n",
      "[93]\ttraining's l1: 0.163202\tvalid_1's l1: 0.164369\n",
      "[94]\ttraining's l1: 0.163412\tvalid_1's l1: 0.164504\n",
      "[95]\ttraining's l1: 0.163382\tvalid_1's l1: 0.164621\n",
      "[96]\ttraining's l1: 0.16316\tvalid_1's l1: 0.164342\n",
      "[97]\ttraining's l1: 0.163129\tvalid_1's l1: 0.164335\n",
      "[98]\ttraining's l1: 0.163076\tvalid_1's l1: 0.164299\n",
      "[99]\ttraining's l1: 0.163018\tvalid_1's l1: 0.164256\n",
      "[100]\ttraining's l1: 0.163001\tvalid_1's l1: 0.164269\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l1: 0.163001\tvalid_1's l1: 0.164269\n",
      "logloss:  0.2671923803643242\n",
      "[1]\ttraining's l1: 0.283321\tvalid_1's l1: 0.283378\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266085\tvalid_1's l1: 0.266235\n",
      "[3]\ttraining's l1: 0.251493\tvalid_1's l1: 0.251697\n",
      "[4]\ttraining's l1: 0.239068\tvalid_1's l1: 0.23933\n",
      "[5]\ttraining's l1: 0.228454\tvalid_1's l1: 0.228768\n",
      "[6]\ttraining's l1: 0.219358\tvalid_1's l1: 0.21972\n",
      "[7]\ttraining's l1: 0.21159\tvalid_1's l1: 0.21201\n",
      "[8]\ttraining's l1: 0.204912\tvalid_1's l1: 0.20538\n",
      "[9]\ttraining's l1: 0.199203\tvalid_1's l1: 0.199724\n",
      "[10]\ttraining's l1: 0.194283\tvalid_1's l1: 0.194829\n",
      "[11]\ttraining's l1: 0.190057\tvalid_1's l1: 0.190628\n",
      "[12]\ttraining's l1: 0.186443\tvalid_1's l1: 0.187035\n",
      "[13]\ttraining's l1: 0.183342\tvalid_1's l1: 0.183959\n",
      "[14]\ttraining's l1: 0.180677\tvalid_1's l1: 0.181316\n",
      "[15]\ttraining's l1: 0.178382\tvalid_1's l1: 0.179044\n",
      "[16]\ttraining's l1: 0.176398\tvalid_1's l1: 0.177076\n",
      "[17]\ttraining's l1: 0.174707\tvalid_1's l1: 0.175402\n",
      "[18]\ttraining's l1: 0.173242\tvalid_1's l1: 0.173959\n",
      "[19]\ttraining's l1: 0.171968\tvalid_1's l1: 0.172713\n",
      "[20]\ttraining's l1: 0.170885\tvalid_1's l1: 0.171655\n",
      "[21]\ttraining's l1: 0.16998\tvalid_1's l1: 0.170771\n",
      "[22]\ttraining's l1: 0.169171\tvalid_1's l1: 0.169976\n",
      "[23]\ttraining's l1: 0.168514\tvalid_1's l1: 0.169337\n",
      "[24]\ttraining's l1: 0.167908\tvalid_1's l1: 0.168746\n",
      "[25]\ttraining's l1: 0.167405\tvalid_1's l1: 0.168253\n",
      "[26]\ttraining's l1: 0.166985\tvalid_1's l1: 0.167857\n",
      "[27]\ttraining's l1: 0.166639\tvalid_1's l1: 0.167515\n",
      "[28]\ttraining's l1: 0.166268\tvalid_1's l1: 0.167159\n",
      "[29]\ttraining's l1: 0.165965\tvalid_1's l1: 0.166868\n",
      "[30]\ttraining's l1: 0.165739\tvalid_1's l1: 0.166653\n",
      "[31]\ttraining's l1: 0.165555\tvalid_1's l1: 0.1665\n",
      "[32]\ttraining's l1: 0.165311\tvalid_1's l1: 0.166273\n",
      "[33]\ttraining's l1: 0.165175\tvalid_1's l1: 0.166207\n",
      "[34]\ttraining's l1: 0.165025\tvalid_1's l1: 0.166086\n",
      "[35]\ttraining's l1: 0.164914\tvalid_1's l1: 0.165988\n",
      "[36]\ttraining's l1: 0.164887\tvalid_1's l1: 0.166021\n",
      "[37]\ttraining's l1: 0.164729\tvalid_1's l1: 0.16586\n",
      "[38]\ttraining's l1: 0.164672\tvalid_1's l1: 0.16582\n",
      "[39]\ttraining's l1: 0.164638\tvalid_1's l1: 0.165788\n",
      "[40]\ttraining's l1: 0.164581\tvalid_1's l1: 0.165747\n",
      "[41]\ttraining's l1: 0.164507\tvalid_1's l1: 0.165681\n",
      "[42]\ttraining's l1: 0.164425\tvalid_1's l1: 0.165594\n",
      "[43]\ttraining's l1: 0.164446\tvalid_1's l1: 0.165528\n",
      "[44]\ttraining's l1: 0.164357\tvalid_1's l1: 0.165487\n",
      "[45]\ttraining's l1: 0.164311\tvalid_1's l1: 0.165447\n",
      "[46]\ttraining's l1: 0.164237\tvalid_1's l1: 0.16535\n",
      "[47]\ttraining's l1: 0.164209\tvalid_1's l1: 0.165436\n",
      "[48]\ttraining's l1: 0.16411\tvalid_1's l1: 0.165328\n",
      "[49]\ttraining's l1: 0.164233\tvalid_1's l1: 0.165379\n",
      "[50]\ttraining's l1: 0.164221\tvalid_1's l1: 0.165484\n",
      "[51]\ttraining's l1: 0.164145\tvalid_1's l1: 0.165491\n",
      "[52]\ttraining's l1: 0.164001\tvalid_1's l1: 0.16534\n",
      "[53]\ttraining's l1: 0.164141\tvalid_1's l1: 0.165499\n",
      "[54]\ttraining's l1: 0.16408\tvalid_1's l1: 0.165439\n",
      "[55]\ttraining's l1: 0.164094\tvalid_1's l1: 0.165405\n",
      "[56]\ttraining's l1: 0.163981\tvalid_1's l1: 0.165356\n",
      "[57]\ttraining's l1: 0.164085\tvalid_1's l1: 0.165431\n",
      "[58]\ttraining's l1: 0.164\tvalid_1's l1: 0.165408\n",
      "[59]\ttraining's l1: 0.163973\tvalid_1's l1: 0.165338\n",
      "[60]\ttraining's l1: 0.163995\tvalid_1's l1: 0.165326\n",
      "[61]\ttraining's l1: 0.163868\tvalid_1's l1: 0.165288\n",
      "[62]\ttraining's l1: 0.163929\tvalid_1's l1: 0.165336\n",
      "[63]\ttraining's l1: 0.163894\tvalid_1's l1: 0.165342\n",
      "[64]\ttraining's l1: 0.163823\tvalid_1's l1: 0.16532\n",
      "[65]\ttraining's l1: 0.163753\tvalid_1's l1: 0.165271\n",
      "[66]\ttraining's l1: 0.163716\tvalid_1's l1: 0.165258\n",
      "[67]\ttraining's l1: 0.163691\tvalid_1's l1: 0.165239\n",
      "[68]\ttraining's l1: 0.163779\tvalid_1's l1: 0.16536\n",
      "[69]\ttraining's l1: 0.163719\tvalid_1's l1: 0.165335\n",
      "[70]\ttraining's l1: 0.16362\tvalid_1's l1: 0.165225\n",
      "[71]\ttraining's l1: 0.163596\tvalid_1's l1: 0.165215\n",
      "[72]\ttraining's l1: 0.163563\tvalid_1's l1: 0.165208\n",
      "[73]\ttraining's l1: 0.163624\tvalid_1's l1: 0.165265\n",
      "[74]\ttraining's l1: 0.163706\tvalid_1's l1: 0.165374\n",
      "[75]\ttraining's l1: 0.1635\tvalid_1's l1: 0.165195\n",
      "[76]\ttraining's l1: 0.163477\tvalid_1's l1: 0.165168\n",
      "[77]\ttraining's l1: 0.163447\tvalid_1's l1: 0.165143\n",
      "[78]\ttraining's l1: 0.163374\tvalid_1's l1: 0.165074\n",
      "[79]\ttraining's l1: 0.163479\tvalid_1's l1: 0.165208\n",
      "[80]\ttraining's l1: 0.163478\tvalid_1's l1: 0.165197\n",
      "[81]\ttraining's l1: 0.163432\tvalid_1's l1: 0.165169\n",
      "[82]\ttraining's l1: 0.163385\tvalid_1's l1: 0.165127\n",
      "[83]\ttraining's l1: 0.163327\tvalid_1's l1: 0.165074\n",
      "[84]\ttraining's l1: 0.163356\tvalid_1's l1: 0.165112\n",
      "[85]\ttraining's l1: 0.164446\tvalid_1's l1: 0.166587\n",
      "[86]\ttraining's l1: 0.163456\tvalid_1's l1: 0.165271\n",
      "[87]\ttraining's l1: 0.163382\tvalid_1's l1: 0.165184\n",
      "[88]\ttraining's l1: 0.163407\tvalid_1's l1: 0.16518\n",
      "[89]\ttraining's l1: 0.163313\tvalid_1's l1: 0.165159\n",
      "[90]\ttraining's l1: 0.163506\tvalid_1's l1: 0.165329\n",
      "[91]\ttraining's l1: 0.163385\tvalid_1's l1: 0.165256\n",
      "[92]\ttraining's l1: 0.163243\tvalid_1's l1: 0.165092\n",
      "[93]\ttraining's l1: 0.163202\tvalid_1's l1: 0.165086\n",
      "[94]\ttraining's l1: 0.163269\tvalid_1's l1: 0.165215\n",
      "[95]\ttraining's l1: 0.163188\tvalid_1's l1: 0.165102\n",
      "[96]\ttraining's l1: 0.163187\tvalid_1's l1: 0.165127\n",
      "[97]\ttraining's l1: 0.163223\tvalid_1's l1: 0.165205\n",
      "[98]\ttraining's l1: 0.163555\tvalid_1's l1: 0.165607\n",
      "[99]\ttraining's l1: 0.163392\tvalid_1's l1: 0.165388\n",
      "[100]\ttraining's l1: 0.163459\tvalid_1's l1: 0.165624\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's l1: 0.163187\tvalid_1's l1: 0.165127\n",
      "logloss:  0.2603813218011073\n",
      "[1]\ttraining's l1: 0.283306\tvalid_1's l1: 0.283398\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266068\tvalid_1's l1: 0.266214\n",
      "[3]\ttraining's l1: 0.25147\tvalid_1's l1: 0.251679\n",
      "[4]\ttraining's l1: 0.23904\tvalid_1's l1: 0.239301\n",
      "[5]\ttraining's l1: 0.228419\tvalid_1's l1: 0.228727\n",
      "[6]\ttraining's l1: 0.219335\tvalid_1's l1: 0.219687\n",
      "[7]\ttraining's l1: 0.211558\tvalid_1's l1: 0.211948\n",
      "[8]\ttraining's l1: 0.204868\tvalid_1's l1: 0.205299\n",
      "[9]\ttraining's l1: 0.199143\tvalid_1's l1: 0.199584\n",
      "[10]\ttraining's l1: 0.194247\tvalid_1's l1: 0.194707\n",
      "[11]\ttraining's l1: 0.190033\tvalid_1's l1: 0.190509\n",
      "[12]\ttraining's l1: 0.186418\tvalid_1's l1: 0.18691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\ttraining's l1: 0.183284\tvalid_1's l1: 0.183799\n",
      "[14]\ttraining's l1: 0.180614\tvalid_1's l1: 0.181162\n",
      "[15]\ttraining's l1: 0.178329\tvalid_1's l1: 0.178894\n",
      "[16]\ttraining's l1: 0.176346\tvalid_1's l1: 0.176927\n",
      "[17]\ttraining's l1: 0.174625\tvalid_1's l1: 0.175231\n",
      "[18]\ttraining's l1: 0.173175\tvalid_1's l1: 0.173796\n",
      "[19]\ttraining's l1: 0.171893\tvalid_1's l1: 0.172525\n",
      "[20]\ttraining's l1: 0.17082\tvalid_1's l1: 0.171468\n",
      "[21]\ttraining's l1: 0.169912\tvalid_1's l1: 0.170581\n",
      "[22]\ttraining's l1: 0.169058\tvalid_1's l1: 0.169742\n",
      "[23]\ttraining's l1: 0.168396\tvalid_1's l1: 0.169093\n",
      "[24]\ttraining's l1: 0.167786\tvalid_1's l1: 0.168506\n",
      "[25]\ttraining's l1: 0.167303\tvalid_1's l1: 0.168032\n",
      "[26]\ttraining's l1: 0.166886\tvalid_1's l1: 0.167634\n",
      "[27]\ttraining's l1: 0.166477\tvalid_1's l1: 0.167235\n",
      "[28]\ttraining's l1: 0.166113\tvalid_1's l1: 0.166898\n",
      "[29]\ttraining's l1: 0.165898\tvalid_1's l1: 0.166699\n",
      "[30]\ttraining's l1: 0.16564\tvalid_1's l1: 0.166459\n",
      "[31]\ttraining's l1: 0.165412\tvalid_1's l1: 0.166259\n",
      "[32]\ttraining's l1: 0.165244\tvalid_1's l1: 0.166109\n",
      "[33]\ttraining's l1: 0.165086\tvalid_1's l1: 0.165973\n",
      "[34]\ttraining's l1: 0.164956\tvalid_1's l1: 0.165885\n",
      "[35]\ttraining's l1: 0.164846\tvalid_1's l1: 0.165809\n",
      "[36]\ttraining's l1: 0.164723\tvalid_1's l1: 0.165724\n",
      "[37]\ttraining's l1: 0.164656\tvalid_1's l1: 0.165696\n",
      "[38]\ttraining's l1: 0.164554\tvalid_1's l1: 0.165622\n",
      "[39]\ttraining's l1: 0.164457\tvalid_1's l1: 0.165528\n",
      "[40]\ttraining's l1: 0.164394\tvalid_1's l1: 0.165478\n",
      "[41]\ttraining's l1: 0.164271\tvalid_1's l1: 0.165381\n",
      "[42]\ttraining's l1: 0.164171\tvalid_1's l1: 0.165302\n",
      "[43]\ttraining's l1: 0.164111\tvalid_1's l1: 0.165267\n",
      "[44]\ttraining's l1: 0.164075\tvalid_1's l1: 0.165283\n",
      "[45]\ttraining's l1: 0.16393\tvalid_1's l1: 0.165115\n",
      "[46]\ttraining's l1: 0.16389\tvalid_1's l1: 0.1651\n",
      "[47]\ttraining's l1: 0.163895\tvalid_1's l1: 0.165184\n",
      "[48]\ttraining's l1: 0.163968\tvalid_1's l1: 0.165414\n",
      "[49]\ttraining's l1: 0.163867\tvalid_1's l1: 0.165296\n",
      "[50]\ttraining's l1: 0.163828\tvalid_1's l1: 0.165253\n",
      "[51]\ttraining's l1: 0.163885\tvalid_1's l1: 0.165268\n",
      "[52]\ttraining's l1: 0.163774\tvalid_1's l1: 0.165205\n",
      "[53]\ttraining's l1: 0.163742\tvalid_1's l1: 0.165191\n",
      "[54]\ttraining's l1: 0.163832\tvalid_1's l1: 0.165237\n",
      "[55]\ttraining's l1: 0.163705\tvalid_1's l1: 0.165182\n",
      "[56]\ttraining's l1: 0.163669\tvalid_1's l1: 0.165113\n",
      "[57]\ttraining's l1: 0.163697\tvalid_1's l1: 0.165145\n",
      "[58]\ttraining's l1: 0.163737\tvalid_1's l1: 0.16518\n",
      "[59]\ttraining's l1: 0.163719\tvalid_1's l1: 0.165207\n",
      "[60]\ttraining's l1: 0.163604\tvalid_1's l1: 0.165118\n",
      "[61]\ttraining's l1: 0.163573\tvalid_1's l1: 0.165097\n",
      "[62]\ttraining's l1: 0.163493\tvalid_1's l1: 0.165044\n",
      "[63]\ttraining's l1: 0.163568\tvalid_1's l1: 0.165201\n",
      "[64]\ttraining's l1: 0.163429\tvalid_1's l1: 0.165093\n",
      "[65]\ttraining's l1: 0.163573\tvalid_1's l1: 0.165329\n",
      "[66]\ttraining's l1: 0.163438\tvalid_1's l1: 0.165208\n",
      "[67]\ttraining's l1: 0.163413\tvalid_1's l1: 0.165201\n",
      "[68]\ttraining's l1: 0.163419\tvalid_1's l1: 0.165189\n",
      "[69]\ttraining's l1: 0.163536\tvalid_1's l1: 0.165255\n",
      "[70]\ttraining's l1: 0.163408\tvalid_1's l1: 0.165207\n",
      "[71]\ttraining's l1: 0.163391\tvalid_1's l1: 0.165223\n",
      "[72]\ttraining's l1: 0.163474\tvalid_1's l1: 0.165385\n",
      "[73]\ttraining's l1: 0.1634\tvalid_1's l1: 0.165298\n",
      "[74]\ttraining's l1: 0.163285\tvalid_1's l1: 0.16514\n",
      "[75]\ttraining's l1: 0.163265\tvalid_1's l1: 0.165142\n",
      "[76]\ttraining's l1: 0.163241\tvalid_1's l1: 0.165147\n",
      "[77]\ttraining's l1: 0.163245\tvalid_1's l1: 0.165138\n",
      "[78]\ttraining's l1: 0.163176\tvalid_1's l1: 0.165148\n",
      "[79]\ttraining's l1: 0.163158\tvalid_1's l1: 0.165148\n",
      "[80]\ttraining's l1: 0.163132\tvalid_1's l1: 0.165153\n",
      "[81]\ttraining's l1: 0.163101\tvalid_1's l1: 0.165158\n",
      "[82]\ttraining's l1: 0.163038\tvalid_1's l1: 0.165125\n",
      "[83]\ttraining's l1: 0.163021\tvalid_1's l1: 0.165123\n",
      "[84]\ttraining's l1: 0.162996\tvalid_1's l1: 0.16514\n",
      "[85]\ttraining's l1: 0.163282\tvalid_1's l1: 0.165561\n",
      "[86]\ttraining's l1: 0.163051\tvalid_1's l1: 0.165304\n",
      "[87]\ttraining's l1: 0.163133\tvalid_1's l1: 0.165378\n",
      "[88]\ttraining's l1: 0.16301\tvalid_1's l1: 0.16525\n",
      "[89]\ttraining's l1: 0.163171\tvalid_1's l1: 0.16548\n",
      "[90]\ttraining's l1: 0.163017\tvalid_1's l1: 0.165244\n",
      "[91]\ttraining's l1: 0.163004\tvalid_1's l1: 0.165294\n",
      "[92]\ttraining's l1: 0.163015\tvalid_1's l1: 0.165248\n",
      "[93]\ttraining's l1: 0.162965\tvalid_1's l1: 0.165293\n",
      "[94]\ttraining's l1: 0.163027\tvalid_1's l1: 0.165273\n",
      "[95]\ttraining's l1: 0.163144\tvalid_1's l1: 0.165453\n",
      "[96]\ttraining's l1: 0.163047\tvalid_1's l1: 0.165476\n",
      "[97]\ttraining's l1: 0.163055\tvalid_1's l1: 0.165415\n",
      "[98]\ttraining's l1: 0.162935\tvalid_1's l1: 0.165355\n",
      "[99]\ttraining's l1: 0.163115\tvalid_1's l1: 0.165574\n",
      "[100]\ttraining's l1: 0.163046\tvalid_1's l1: 0.165495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's l1: 0.162935\tvalid_1's l1: 0.165355\n",
      "logloss:  0.2675606382746851\n",
      "[1]\ttraining's l1: 0.283289\tvalid_1's l1: 0.283518\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266047\tvalid_1's l1: 0.266432\n",
      "[3]\ttraining's l1: 0.251447\tvalid_1's l1: 0.251932\n",
      "[4]\ttraining's l1: 0.239011\tvalid_1's l1: 0.239591\n",
      "[5]\ttraining's l1: 0.228381\tvalid_1's l1: 0.229044\n",
      "[6]\ttraining's l1: 0.219282\tvalid_1's l1: 0.220023\n",
      "[7]\ttraining's l1: 0.211502\tvalid_1's l1: 0.212297\n",
      "[8]\ttraining's l1: 0.204818\tvalid_1's l1: 0.205661\n",
      "[9]\ttraining's l1: 0.199085\tvalid_1's l1: 0.199966\n",
      "[10]\ttraining's l1: 0.194177\tvalid_1's l1: 0.195086\n",
      "[11]\ttraining's l1: 0.189972\tvalid_1's l1: 0.190918\n",
      "[12]\ttraining's l1: 0.186343\tvalid_1's l1: 0.187324\n",
      "[13]\ttraining's l1: 0.183216\tvalid_1's l1: 0.184232\n",
      "[14]\ttraining's l1: 0.180538\tvalid_1's l1: 0.1816\n",
      "[15]\ttraining's l1: 0.178232\tvalid_1's l1: 0.179323\n",
      "[16]\ttraining's l1: 0.176242\tvalid_1's l1: 0.177361\n",
      "[17]\ttraining's l1: 0.174539\tvalid_1's l1: 0.17568\n",
      "[18]\ttraining's l1: 0.173077\tvalid_1's l1: 0.174241\n",
      "[19]\ttraining's l1: 0.171843\tvalid_1's l1: 0.173031\n",
      "[20]\ttraining's l1: 0.170751\tvalid_1's l1: 0.171945\n",
      "[21]\ttraining's l1: 0.169857\tvalid_1's l1: 0.171063\n",
      "[22]\ttraining's l1: 0.169042\tvalid_1's l1: 0.170284\n",
      "[23]\ttraining's l1: 0.168353\tvalid_1's l1: 0.16962\n",
      "[24]\ttraining's l1: 0.167763\tvalid_1's l1: 0.169044\n",
      "[25]\ttraining's l1: 0.167253\tvalid_1's l1: 0.168559\n",
      "[26]\ttraining's l1: 0.166799\tvalid_1's l1: 0.168115\n",
      "[27]\ttraining's l1: 0.166384\tvalid_1's l1: 0.167717\n",
      "[28]\ttraining's l1: 0.166061\tvalid_1's l1: 0.167413\n",
      "[29]\ttraining's l1: 0.165788\tvalid_1's l1: 0.167152\n",
      "[30]\ttraining's l1: 0.165557\tvalid_1's l1: 0.166923\n",
      "[31]\ttraining's l1: 0.16533\tvalid_1's l1: 0.166731\n",
      "[32]\ttraining's l1: 0.165132\tvalid_1's l1: 0.166558\n",
      "[33]\ttraining's l1: 0.164958\tvalid_1's l1: 0.166397\n",
      "[34]\ttraining's l1: 0.164827\tvalid_1's l1: 0.166289\n",
      "[35]\ttraining's l1: 0.16474\tvalid_1's l1: 0.166274\n",
      "[36]\ttraining's l1: 0.164697\tvalid_1's l1: 0.166202\n",
      "[37]\ttraining's l1: 0.164586\tvalid_1's l1: 0.166101\n",
      "[38]\ttraining's l1: 0.164501\tvalid_1's l1: 0.166054\n",
      "[39]\ttraining's l1: 0.164368\tvalid_1's l1: 0.165935\n",
      "[40]\ttraining's l1: 0.164306\tvalid_1's l1: 0.165898\n",
      "[41]\ttraining's l1: 0.164236\tvalid_1's l1: 0.165849\n",
      "[42]\ttraining's l1: 0.164224\tvalid_1's l1: 0.165805\n",
      "[43]\ttraining's l1: 0.164163\tvalid_1's l1: 0.165764\n",
      "[44]\ttraining's l1: 0.164117\tvalid_1's l1: 0.165737\n",
      "[45]\ttraining's l1: 0.164207\tvalid_1's l1: 0.165894\n",
      "[46]\ttraining's l1: 0.164165\tvalid_1's l1: 0.165898\n",
      "[47]\ttraining's l1: 0.164186\tvalid_1's l1: 0.165912\n",
      "[48]\ttraining's l1: 0.164075\tvalid_1's l1: 0.165784\n",
      "[49]\ttraining's l1: 0.164269\tvalid_1's l1: 0.166008\n",
      "[50]\ttraining's l1: 0.164099\tvalid_1's l1: 0.165895\n",
      "[51]\ttraining's l1: 0.164334\tvalid_1's l1: 0.166088\n",
      "[52]\ttraining's l1: 0.16407\tvalid_1's l1: 0.16589\n",
      "[53]\ttraining's l1: 0.16402\tvalid_1's l1: 0.165843\n",
      "[54]\ttraining's l1: 0.163982\tvalid_1's l1: 0.165835\n",
      "[55]\ttraining's l1: 0.164185\tvalid_1's l1: 0.16605\n",
      "[56]\ttraining's l1: 0.16385\tvalid_1's l1: 0.165708\n",
      "[57]\ttraining's l1: 0.163806\tvalid_1's l1: 0.165662\n",
      "[58]\ttraining's l1: 0.163862\tvalid_1's l1: 0.16569\n",
      "[59]\ttraining's l1: 0.163827\tvalid_1's l1: 0.165729\n",
      "[60]\ttraining's l1: 0.163941\tvalid_1's l1: 0.165832\n",
      "[61]\ttraining's l1: 0.163698\tvalid_1's l1: 0.165741\n",
      "[62]\ttraining's l1: 0.163662\tvalid_1's l1: 0.16574\n",
      "[63]\ttraining's l1: 0.16363\tvalid_1's l1: 0.165755\n",
      "[64]\ttraining's l1: 0.163687\tvalid_1's l1: 0.165721\n",
      "[65]\ttraining's l1: 0.163735\tvalid_1's l1: 0.165851\n",
      "[66]\ttraining's l1: 0.163663\tvalid_1's l1: 0.165796\n",
      "[67]\ttraining's l1: 0.16365\tvalid_1's l1: 0.165848\n",
      "[68]\ttraining's l1: 0.163567\tvalid_1's l1: 0.165789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\ttraining's l1: 0.163505\tvalid_1's l1: 0.165704\n",
      "[70]\ttraining's l1: 0.163418\tvalid_1's l1: 0.165646\n",
      "[71]\ttraining's l1: 0.163654\tvalid_1's l1: 0.166034\n",
      "[72]\ttraining's l1: 0.16395\tvalid_1's l1: 0.166489\n",
      "[73]\ttraining's l1: 0.163521\tvalid_1's l1: 0.165924\n",
      "[74]\ttraining's l1: 0.163811\tvalid_1's l1: 0.166163\n",
      "[75]\ttraining's l1: 0.163549\tvalid_1's l1: 0.16602\n",
      "[76]\ttraining's l1: 0.163475\tvalid_1's l1: 0.16602\n",
      "[77]\ttraining's l1: 0.163476\tvalid_1's l1: 0.166044\n",
      "[78]\ttraining's l1: 0.163379\tvalid_1's l1: 0.165931\n",
      "[79]\ttraining's l1: 0.163534\tvalid_1's l1: 0.166063\n",
      "[80]\ttraining's l1: 0.163353\tvalid_1's l1: 0.165941\n",
      "[81]\ttraining's l1: 0.163407\tvalid_1's l1: 0.165995\n",
      "[82]\ttraining's l1: 0.163301\tvalid_1's l1: 0.165926\n",
      "[83]\ttraining's l1: 0.163614\tvalid_1's l1: 0.166078\n",
      "[84]\ttraining's l1: 0.163243\tvalid_1's l1: 0.165877\n",
      "[85]\ttraining's l1: 0.163266\tvalid_1's l1: 0.165915\n",
      "[86]\ttraining's l1: 0.163201\tvalid_1's l1: 0.165873\n",
      "[87]\ttraining's l1: 0.163588\tvalid_1's l1: 0.166258\n",
      "[88]\ttraining's l1: 0.163326\tvalid_1's l1: 0.166028\n",
      "[89]\ttraining's l1: 0.163248\tvalid_1's l1: 0.166015\n",
      "[90]\ttraining's l1: 0.163256\tvalid_1's l1: 0.166046\n",
      "[91]\ttraining's l1: 0.163219\tvalid_1's l1: 0.165928\n",
      "[92]\ttraining's l1: 0.163456\tvalid_1's l1: 0.166162\n",
      "[93]\ttraining's l1: 0.163214\tvalid_1's l1: 0.165888\n",
      "[94]\ttraining's l1: 0.163201\tvalid_1's l1: 0.165971\n",
      "[95]\ttraining's l1: 0.163197\tvalid_1's l1: 0.165947\n",
      "[96]\ttraining's l1: 0.163142\tvalid_1's l1: 0.165949\n",
      "[97]\ttraining's l1: 0.16331\tvalid_1's l1: 0.166139\n",
      "[98]\ttraining's l1: 0.1631\tvalid_1's l1: 0.165932\n",
      "[99]\ttraining's l1: 0.163093\tvalid_1's l1: 0.165928\n",
      "[100]\ttraining's l1: 0.163087\tvalid_1's l1: 0.165946\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l1: 0.163087\tvalid_1's l1: 0.165946\n",
      "logloss:  0.2743461764526199\n",
      "[1]\ttraining's l1: 0.283324\tvalid_1's l1: 0.283395\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266093\tvalid_1's l1: 0.266183\n",
      "[3]\ttraining's l1: 0.251513\tvalid_1's l1: 0.251652\n",
      "[4]\ttraining's l1: 0.239106\tvalid_1's l1: 0.23929\n",
      "[5]\ttraining's l1: 0.228491\tvalid_1's l1: 0.22871\n",
      "[6]\ttraining's l1: 0.219427\tvalid_1's l1: 0.219678\n",
      "[7]\ttraining's l1: 0.211635\tvalid_1's l1: 0.211922\n",
      "[8]\ttraining's l1: 0.204956\tvalid_1's l1: 0.205271\n",
      "[9]\ttraining's l1: 0.199216\tvalid_1's l1: 0.199554\n",
      "[10]\ttraining's l1: 0.194303\tvalid_1's l1: 0.194669\n",
      "[11]\ttraining's l1: 0.190103\tvalid_1's l1: 0.190492\n",
      "[12]\ttraining's l1: 0.186498\tvalid_1's l1: 0.186903\n",
      "[13]\ttraining's l1: 0.183373\tvalid_1's l1: 0.183797\n",
      "[14]\ttraining's l1: 0.180721\tvalid_1's l1: 0.181154\n",
      "[15]\ttraining's l1: 0.178412\tvalid_1's l1: 0.178869\n",
      "[16]\ttraining's l1: 0.176451\tvalid_1's l1: 0.17693\n",
      "[17]\ttraining's l1: 0.17476\tvalid_1's l1: 0.175253\n",
      "[18]\ttraining's l1: 0.173309\tvalid_1's l1: 0.173813\n",
      "[19]\ttraining's l1: 0.172025\tvalid_1's l1: 0.172551\n",
      "[20]\ttraining's l1: 0.170968\tvalid_1's l1: 0.171526\n",
      "[21]\ttraining's l1: 0.170031\tvalid_1's l1: 0.170602\n",
      "[22]\ttraining's l1: 0.169217\tvalid_1's l1: 0.169822\n",
      "[23]\ttraining's l1: 0.168513\tvalid_1's l1: 0.169114\n",
      "[24]\ttraining's l1: 0.167923\tvalid_1's l1: 0.168543\n",
      "[25]\ttraining's l1: 0.167389\tvalid_1's l1: 0.168022\n",
      "[26]\ttraining's l1: 0.16699\tvalid_1's l1: 0.167649\n",
      "[27]\ttraining's l1: 0.166634\tvalid_1's l1: 0.167317\n",
      "[28]\ttraining's l1: 0.166255\tvalid_1's l1: 0.166947\n",
      "[29]\ttraining's l1: 0.166026\tvalid_1's l1: 0.166739\n",
      "[30]\ttraining's l1: 0.165812\tvalid_1's l1: 0.166534\n",
      "[31]\ttraining's l1: 0.165612\tvalid_1's l1: 0.166366\n",
      "[32]\ttraining's l1: 0.165425\tvalid_1's l1: 0.166213\n",
      "[33]\ttraining's l1: 0.165311\tvalid_1's l1: 0.166079\n",
      "[34]\ttraining's l1: 0.165128\tvalid_1's l1: 0.165927\n",
      "[35]\ttraining's l1: 0.165005\tvalid_1's l1: 0.165853\n",
      "[36]\ttraining's l1: 0.164879\tvalid_1's l1: 0.16574\n",
      "[37]\ttraining's l1: 0.164824\tvalid_1's l1: 0.165695\n",
      "[38]\ttraining's l1: 0.164741\tvalid_1's l1: 0.165622\n",
      "[39]\ttraining's l1: 0.164648\tvalid_1's l1: 0.165547\n",
      "[40]\ttraining's l1: 0.164671\tvalid_1's l1: 0.165558\n",
      "[41]\ttraining's l1: 0.164604\tvalid_1's l1: 0.165583\n",
      "[42]\ttraining's l1: 0.164541\tvalid_1's l1: 0.165513\n",
      "[43]\ttraining's l1: 0.164436\tvalid_1's l1: 0.165432\n",
      "[44]\ttraining's l1: 0.164539\tvalid_1's l1: 0.165557\n",
      "[45]\ttraining's l1: 0.16454\tvalid_1's l1: 0.165614\n",
      "[46]\ttraining's l1: 0.164439\tvalid_1's l1: 0.165495\n",
      "[47]\ttraining's l1: 0.164343\tvalid_1's l1: 0.165434\n",
      "[48]\ttraining's l1: 0.164273\tvalid_1's l1: 0.165373\n",
      "[49]\ttraining's l1: 0.16417\tvalid_1's l1: 0.165277\n",
      "[50]\ttraining's l1: 0.164261\tvalid_1's l1: 0.1653\n",
      "[51]\ttraining's l1: 0.164124\tvalid_1's l1: 0.165214\n",
      "[52]\ttraining's l1: 0.164122\tvalid_1's l1: 0.165267\n",
      "[53]\ttraining's l1: 0.163986\tvalid_1's l1: 0.165126\n",
      "[54]\ttraining's l1: 0.163992\tvalid_1's l1: 0.165167\n",
      "[55]\ttraining's l1: 0.163941\tvalid_1's l1: 0.165138\n",
      "[56]\ttraining's l1: 0.163913\tvalid_1's l1: 0.165147\n",
      "[57]\ttraining's l1: 0.163867\tvalid_1's l1: 0.165136\n",
      "[58]\ttraining's l1: 0.163818\tvalid_1's l1: 0.165123\n",
      "[59]\ttraining's l1: 0.163787\tvalid_1's l1: 0.165101\n",
      "[60]\ttraining's l1: 0.163847\tvalid_1's l1: 0.165136\n",
      "[61]\ttraining's l1: 0.163811\tvalid_1's l1: 0.165127\n",
      "[62]\ttraining's l1: 0.163796\tvalid_1's l1: 0.165093\n",
      "[63]\ttraining's l1: 0.1637\tvalid_1's l1: 0.165046\n",
      "[64]\ttraining's l1: 0.163765\tvalid_1's l1: 0.165047\n",
      "[65]\ttraining's l1: 0.163664\tvalid_1's l1: 0.165018\n",
      "[66]\ttraining's l1: 0.163596\tvalid_1's l1: 0.164943\n",
      "[67]\ttraining's l1: 0.16356\tvalid_1's l1: 0.16493\n",
      "[68]\ttraining's l1: 0.163486\tvalid_1's l1: 0.164868\n",
      "[69]\ttraining's l1: 0.163585\tvalid_1's l1: 0.164976\n",
      "[70]\ttraining's l1: 0.163512\tvalid_1's l1: 0.164965\n",
      "[71]\ttraining's l1: 0.163477\tvalid_1's l1: 0.164867\n",
      "[72]\ttraining's l1: 0.16349\tvalid_1's l1: 0.164865\n",
      "[73]\ttraining's l1: 0.16337\tvalid_1's l1: 0.164835\n",
      "[74]\ttraining's l1: 0.163347\tvalid_1's l1: 0.164823\n",
      "[75]\ttraining's l1: 0.163318\tvalid_1's l1: 0.16482\n",
      "[76]\ttraining's l1: 0.163583\tvalid_1's l1: 0.165024\n",
      "[77]\ttraining's l1: 0.163393\tvalid_1's l1: 0.164917\n",
      "[78]\ttraining's l1: 0.163311\tvalid_1's l1: 0.164869\n",
      "[79]\ttraining's l1: 0.163281\tvalid_1's l1: 0.164854\n",
      "[80]\ttraining's l1: 0.163384\tvalid_1's l1: 0.164918\n",
      "[81]\ttraining's l1: 0.163509\tvalid_1's l1: 0.165013\n",
      "[82]\ttraining's l1: 0.163479\tvalid_1's l1: 0.165076\n",
      "[83]\ttraining's l1: 0.163279\tvalid_1's l1: 0.164891\n",
      "[84]\ttraining's l1: 0.163773\tvalid_1's l1: 0.165241\n",
      "[85]\ttraining's l1: 0.163332\tvalid_1's l1: 0.164957\n",
      "[86]\ttraining's l1: 0.163638\tvalid_1's l1: 0.165522\n",
      "[87]\ttraining's l1: 0.163221\tvalid_1's l1: 0.164924\n",
      "[88]\ttraining's l1: 0.163315\tvalid_1's l1: 0.165138\n",
      "[89]\ttraining's l1: 0.163886\tvalid_1's l1: 0.165458\n",
      "[90]\ttraining's l1: 0.163379\tvalid_1's l1: 0.16512\n",
      "[91]\ttraining's l1: 0.163498\tvalid_1's l1: 0.165317\n",
      "[92]\ttraining's l1: 0.163382\tvalid_1's l1: 0.165126\n",
      "[93]\ttraining's l1: 0.163211\tvalid_1's l1: 0.164924\n",
      "[94]\ttraining's l1: 0.163316\tvalid_1's l1: 0.165004\n",
      "[95]\ttraining's l1: 0.163227\tvalid_1's l1: 0.165015\n",
      "[96]\ttraining's l1: 0.16317\tvalid_1's l1: 0.164927\n",
      "[97]\ttraining's l1: 0.163726\tvalid_1's l1: 0.165733\n",
      "[98]\ttraining's l1: 0.163308\tvalid_1's l1: 0.165238\n",
      "[99]\ttraining's l1: 0.163257\tvalid_1's l1: 0.16506\n",
      "[100]\ttraining's l1: 0.163152\tvalid_1's l1: 0.16493\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's l1: 0.163152\tvalid_1's l1: 0.16493\n",
      "logloss:  0.2637782142849858\n",
      "[1]\ttraining's l1: 0.283342\tvalid_1's l1: 0.283228\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[2]\ttraining's l1: 0.266131\tvalid_1's l1: 0.265923\n",
      "[3]\ttraining's l1: 0.251551\tvalid_1's l1: 0.251281\n",
      "[4]\ttraining's l1: 0.239136\tvalid_1's l1: 0.238808\n",
      "[5]\ttraining's l1: 0.228538\tvalid_1's l1: 0.22816\n",
      "[6]\ttraining's l1: 0.219468\tvalid_1's l1: 0.219049\n",
      "[7]\ttraining's l1: 0.211705\tvalid_1's l1: 0.211247\n",
      "[8]\ttraining's l1: 0.205029\tvalid_1's l1: 0.204561\n",
      "[9]\ttraining's l1: 0.199322\tvalid_1's l1: 0.198846\n",
      "[10]\ttraining's l1: 0.194407\tvalid_1's l1: 0.193915\n",
      "[11]\ttraining's l1: 0.190207\tvalid_1's l1: 0.189711\n",
      "[12]\ttraining's l1: 0.186578\tvalid_1's l1: 0.186079\n",
      "[13]\ttraining's l1: 0.183442\tvalid_1's l1: 0.182934\n",
      "[14]\ttraining's l1: 0.18076\tvalid_1's l1: 0.180251\n",
      "[15]\ttraining's l1: 0.178473\tvalid_1's l1: 0.177979\n",
      "[16]\ttraining's l1: 0.176491\tvalid_1's l1: 0.175986\n",
      "[17]\ttraining's l1: 0.174792\tvalid_1's l1: 0.174284\n",
      "[18]\ttraining's l1: 0.173312\tvalid_1's l1: 0.172827\n",
      "[19]\ttraining's l1: 0.172064\tvalid_1's l1: 0.171583\n",
      "[20]\ttraining's l1: 0.170988\tvalid_1's l1: 0.170516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\ttraining's l1: 0.170049\tvalid_1's l1: 0.169582\n",
      "[22]\ttraining's l1: 0.169213\tvalid_1's l1: 0.168765\n",
      "[23]\ttraining's l1: 0.16853\tvalid_1's l1: 0.168093\n",
      "[24]\ttraining's l1: 0.167934\tvalid_1's l1: 0.167523\n",
      "[25]\ttraining's l1: 0.167433\tvalid_1's l1: 0.16704\n",
      "[26]\ttraining's l1: 0.166979\tvalid_1's l1: 0.166598\n",
      "[27]\ttraining's l1: 0.166625\tvalid_1's l1: 0.166252\n",
      "[28]\ttraining's l1: 0.16631\tvalid_1's l1: 0.16595\n",
      "[29]\ttraining's l1: 0.166037\tvalid_1's l1: 0.165688\n",
      "[30]\ttraining's l1: 0.165828\tvalid_1's l1: 0.165489\n",
      "[31]\ttraining's l1: 0.1656\tvalid_1's l1: 0.165291\n",
      "[32]\ttraining's l1: 0.165413\tvalid_1's l1: 0.165118\n",
      "[33]\ttraining's l1: 0.165228\tvalid_1's l1: 0.164939\n",
      "[34]\ttraining's l1: 0.165089\tvalid_1's l1: 0.164828\n",
      "[35]\ttraining's l1: 0.165021\tvalid_1's l1: 0.164763\n",
      "[36]\ttraining's l1: 0.164898\tvalid_1's l1: 0.164657\n",
      "[37]\ttraining's l1: 0.164815\tvalid_1's l1: 0.16459\n",
      "[38]\ttraining's l1: 0.164711\tvalid_1's l1: 0.164494\n",
      "[39]\ttraining's l1: 0.164632\tvalid_1's l1: 0.164433\n",
      "[40]\ttraining's l1: 0.164492\tvalid_1's l1: 0.16431\n",
      "[41]\ttraining's l1: 0.164473\tvalid_1's l1: 0.164294\n",
      "[42]\ttraining's l1: 0.164353\tvalid_1's l1: 0.164194\n",
      "[43]\ttraining's l1: 0.164453\tvalid_1's l1: 0.164333\n",
      "[44]\ttraining's l1: 0.164328\tvalid_1's l1: 0.164244\n",
      "[45]\ttraining's l1: 0.164374\tvalid_1's l1: 0.16432\n",
      "[46]\ttraining's l1: 0.164237\tvalid_1's l1: 0.164198\n",
      "[47]\ttraining's l1: 0.164295\tvalid_1's l1: 0.164335\n",
      "[48]\ttraining's l1: 0.164212\tvalid_1's l1: 0.164276\n",
      "[49]\ttraining's l1: 0.164212\tvalid_1's l1: 0.164286\n",
      "[50]\ttraining's l1: 0.164131\tvalid_1's l1: 0.164255\n",
      "[51]\ttraining's l1: 0.164137\tvalid_1's l1: 0.164272\n",
      "[52]\ttraining's l1: 0.164136\tvalid_1's l1: 0.164257\n",
      "[53]\ttraining's l1: 0.164226\tvalid_1's l1: 0.164408\n",
      "[54]\ttraining's l1: 0.164115\tvalid_1's l1: 0.164299\n",
      "[55]\ttraining's l1: 0.164245\tvalid_1's l1: 0.164513\n",
      "[56]\ttraining's l1: 0.164093\tvalid_1's l1: 0.164311\n",
      "[57]\ttraining's l1: 0.164192\tvalid_1's l1: 0.164347\n",
      "[58]\ttraining's l1: 0.164204\tvalid_1's l1: 0.164422\n",
      "[59]\ttraining's l1: 0.164106\tvalid_1's l1: 0.164448\n",
      "[60]\ttraining's l1: 0.164106\tvalid_1's l1: 0.164448\n",
      "[61]\ttraining's l1: 0.164186\tvalid_1's l1: 0.164604\n",
      "[62]\ttraining's l1: 0.164115\tvalid_1's l1: 0.164438\n",
      "[63]\ttraining's l1: 0.164192\tvalid_1's l1: 0.164627\n",
      "[64]\ttraining's l1: 0.164055\tvalid_1's l1: 0.164532\n",
      "[65]\ttraining's l1: 0.164026\tvalid_1's l1: 0.164456\n",
      "[66]\ttraining's l1: 0.164022\tvalid_1's l1: 0.164536\n",
      "[67]\ttraining's l1: 0.163994\tvalid_1's l1: 0.164581\n",
      "[68]\ttraining's l1: 0.163944\tvalid_1's l1: 0.164435\n",
      "[69]\ttraining's l1: 0.163899\tvalid_1's l1: 0.164419\n",
      "[70]\ttraining's l1: 0.163813\tvalid_1's l1: 0.164352\n",
      "[71]\ttraining's l1: 0.163746\tvalid_1's l1: 0.164327\n",
      "[72]\ttraining's l1: 0.163712\tvalid_1's l1: 0.164299\n",
      "[73]\ttraining's l1: 0.163682\tvalid_1's l1: 0.164304\n",
      "[74]\ttraining's l1: 0.163652\tvalid_1's l1: 0.164315\n",
      "[75]\ttraining's l1: 0.163668\tvalid_1's l1: 0.164386\n",
      "[76]\ttraining's l1: 0.163615\tvalid_1's l1: 0.164351\n",
      "[77]\ttraining's l1: 0.163821\tvalid_1's l1: 0.164544\n",
      "[78]\ttraining's l1: 0.163693\tvalid_1's l1: 0.164532\n",
      "[79]\ttraining's l1: 0.163553\tvalid_1's l1: 0.164374\n",
      "[80]\ttraining's l1: 0.163784\tvalid_1's l1: 0.16459\n",
      "[81]\ttraining's l1: 0.163706\tvalid_1's l1: 0.164492\n",
      "[82]\ttraining's l1: 0.163646\tvalid_1's l1: 0.164476\n",
      "[83]\ttraining's l1: 0.163669\tvalid_1's l1: 0.164505\n",
      "[84]\ttraining's l1: 0.163647\tvalid_1's l1: 0.164514\n",
      "[85]\ttraining's l1: 0.163607\tvalid_1's l1: 0.164574\n",
      "[86]\ttraining's l1: 0.163606\tvalid_1's l1: 0.164577\n",
      "[87]\ttraining's l1: 0.163527\tvalid_1's l1: 0.164521\n",
      "[88]\ttraining's l1: 0.163582\tvalid_1's l1: 0.164537\n",
      "[89]\ttraining's l1: 0.163605\tvalid_1's l1: 0.164528\n",
      "[90]\ttraining's l1: 0.163733\tvalid_1's l1: 0.164891\n",
      "[91]\ttraining's l1: 0.163463\tvalid_1's l1: 0.16446\n",
      "[92]\ttraining's l1: 0.163464\tvalid_1's l1: 0.164456\n",
      "[93]\ttraining's l1: 0.163689\tvalid_1's l1: 0.164751\n",
      "[94]\ttraining's l1: 0.16359\tvalid_1's l1: 0.16457\n",
      "[95]\ttraining's l1: 0.163473\tvalid_1's l1: 0.164377\n",
      "[96]\ttraining's l1: 0.163361\tvalid_1's l1: 0.164315\n",
      "[97]\ttraining's l1: 0.16333\tvalid_1's l1: 0.164304\n",
      "[98]\ttraining's l1: 0.163444\tvalid_1's l1: 0.164549\n",
      "[99]\ttraining's l1: 0.163353\tvalid_1's l1: 0.164466\n",
      "[100]\ttraining's l1: 0.163528\tvalid_1's l1: 0.164651\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's l1: 0.16333\tvalid_1's l1: 0.164304\n",
      "logloss:  0.2612596798311698\n"
     ]
    }
   ],
   "source": [
    "errlgb = []\n",
    "y_pred_totlgb = []\n",
    "\n",
    "fold = StratifiedKFold(n_splits=10, shuffle=False, random_state=42)\n",
    "\n",
    "for train_index,test_index in fold.split(x,y):\n",
    "    x_train,x_test=x[train_index], x[test_index]\n",
    "    y_train,y_test=y[train_index], y[test_index]\n",
    "    \n",
    "    estimator=lgb.LGBMClassifier(learning_rate=0.150, metric='l1', \n",
    "                                early_stopping_rounds =200, \n",
    "                                eval_metric ='binary_logloss',max_depth = -1,\n",
    "                                n_estimators =100)\n",
    "    \n",
    "    estimator.fit(x_train,y_train, eval_set=[(x_train,y_train), (x_test,y_test)])\n",
    "    \n",
    "    y_pred = estimator.predict_proba(x_test)[:,1] \n",
    "\n",
    "    print(\"logloss: \",log_loss(y_test, y_pred))\n",
    "    \n",
    "    errlgb.append(log_loss(y_test, y_pred))\n",
    "    \n",
    "    preds=estimator.predict_proba(Xtest)[:,1]\n",
    "    y_pred_totlgb.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77955824, 0.67123499, 0.10704709, ..., 0.03661029, 0.00782681,\n",
       "       0.04354306])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=np.mean(y_pred_totlgb,0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>CHURN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af900d87e73b7ff6509d2203df4704a98aa5f2a6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5335efd940280b82143272275637d1e65d37eadb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a581f4fa08677c26f83f643248c667e241043086</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64f67177d0775262b8087a9e2e3b8061b6324ae6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d6009a4594c4be22449b8d9cc01a0bcea98faea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    user_id  CHURN\n",
       "0  af900d87e73b7ff6509d2203df4704a98aa5f2a6      0\n",
       "1  5335efd940280b82143272275637d1e65d37eadb      0\n",
       "2  a581f4fa08677c26f83f643248c667e241043086      0\n",
       "3  64f67177d0775262b8087a9e2e3b8061b6324ae6      0\n",
       "4  0d6009a4594c4be22449b8d9cc01a0bcea98faea      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>CHURN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af900d87e73b7ff6509d2203df4704a98aa5f2a6</td>\n",
       "      <td>7.795582e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5335efd940280b82143272275637d1e65d37eadb</td>\n",
       "      <td>6.712350e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a581f4fa08677c26f83f643248c667e241043086</td>\n",
       "      <td>1.070471e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64f67177d0775262b8087a9e2e3b8061b6324ae6</td>\n",
       "      <td>1.003956e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d6009a4594c4be22449b8d9cc01a0bcea98faea</td>\n",
       "      <td>8.226380e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    user_id         CHURN\n",
       "0  af900d87e73b7ff6509d2203df4704a98aa5f2a6  7.795582e-01\n",
       "1  5335efd940280b82143272275637d1e65d37eadb  6.712350e-01\n",
       "2  a581f4fa08677c26f83f643248c667e241043086  1.070471e-01\n",
       "3  64f67177d0775262b8087a9e2e3b8061b6324ae6  1.003956e-03\n",
       "4  0d6009a4594c4be22449b8d9cc01a0bcea98faea  8.226380e-07"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['CHURN'] = y_pred\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('SUBBBB5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2657105763519974"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglosss=np.mean(errlgb,0)\n",
    "loglosss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
